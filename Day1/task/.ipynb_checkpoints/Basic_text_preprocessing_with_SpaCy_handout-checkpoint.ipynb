{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQGvpTjNl7q-"
   },
   "source": [
    "# Using the SpaCy pipeline\n",
    "\n",
    "This task is aiming to demonstrate the tokenization capabilites of [SpaCy](https://spacy.io/), as well as to serve as an introduction to the pipeline's capabilities combined with [rule based matching](https://spacy.io/usage/rule-based-matching).\n",
    "\n",
    "Our goal will be to process the demonstration text, as well as to correct for some peculiarities, like special pronunciation marks, wide-spread abbreviations and foreign language insertions into our text.\n",
    "\n",
    "It is mandatory, to stick to SpaCy based pipeline operations so as to make our analysis reproducible by running the pipeline on other texts presumably coming from the same corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hghIi-BYl7rH"
   },
   "source": [
    "## Our demonstration text\n",
    "\n",
    "Original from [Deutsche Sprache](https://de.wikipedia.org/wiki/Deutsche_Sprache) Wikipedia entry - with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:03.801971Z",
     "start_time": "2019-11-08T11:44:03.798661Z"
    },
    "id": "dIFE2QzAl7rI"
   },
   "outputs": [],
   "source": [
    "text= '''Die deutsche Sprache bzw. Deutsch ([dɔʏ̯t͡ʃ]; abgekürzt dt. oder dtsch.) ist eine westgermanische Sprache.\n",
    "\n",
    "And this is an English sentence inbetween.\n",
    "\n",
    "Ihr Sprachraum umfasst Deutschland, Österreich, die Deutschschweiz, Liechtenstein, Luxemburg, Ostbelgien, Südtirol, das Elsass und Lothringen sowie Nordschleswig. Außerdem ist sie eine Minderheitensprache in einigen europäischen und außereuropäischen Ländern, z. B. in Rumänien und Südafrika, sowie Nationalsprache im afrikanischen Namibia.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8iXMRefl7rL"
   },
   "source": [
    "## Basic usage\n",
    "\n",
    "After installing SpaCy, let us demonstrate it's basic usage by analysing our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:04.978496Z",
     "start_time": "2019-11-08T11:44:03.810849Z"
    },
    "id": "7uI7qyJHl7rM"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tabulate >> /dev/null\n",
    "!pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_g1Rg4ol7rN"
   },
   "outputs": [],
   "source": [
    "# Ok, we installed SpaCy, but do we have a model for German?\n",
    "# Something has to be done here to get it!\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sB5vxwBol7rN"
   },
   "outputs": [],
   "source": [
    "# If after the action above, the German model does not load, you may have to restart the runtime.\n",
    "# (in Colab it can be so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.365430Z",
     "start_time": "2019-11-08T11:44:04.981233Z"
    },
    "id": "2_YmX5avl7rO"
   },
   "outputs": [],
   "source": [
    "# Please do the appropriate imports for SpaCy and it's rule based Matcher class and Language!\n",
    "\n",
    "....\n",
    "\n",
    "# Please don't forget to instantiate the language model that we will use later on for analysis\n",
    "\n",
    "nlp=...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.385990Z",
     "start_time": "2019-11-08T11:44:06.366784Z"
    },
    "id": "4Pw1FygQl7rP"
   },
   "outputs": [],
   "source": [
    "# And please use the model to analyse the text from above!\n",
    "\n",
    "doc=...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LPjwwHnl7rQ"
   },
   "source": [
    "### Helper functions for nice printout\n",
    "\n",
    "We just define some helper functions for nice printout. Nothing to do here, except to observe the ways one can iterate over a corpus or sentence, as well as the nice output of [Tabulate](https://bitbucket.org/astanin/python-tabulate/src/master/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.395057Z",
     "start_time": "2019-11-08T11:44:06.387362Z"
    },
    "id": "L-936Bpol7rR"
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_sentences(doc):\n",
    "    for sentence in doc.sents:\n",
    "        print(sentence,\"\\n\")\n",
    "\n",
    "def print_tokens_for_sentence(doc,sentence_num, stopwords=False):\n",
    "    attribs=[]\n",
    "    for token in list(doc.sents)[sentence_num]:\n",
    "        if token.has_extension(\"is_lemma_stop\"):\n",
    "            if stopwords and token._.is_lemma_stop:\n",
    "                pass\n",
    "            else:\n",
    "                attribs.append([token.text, token.lemma_, token.pos_])\n",
    "        else:\n",
    "            attribs.append([token.text, token.lemma_, token.pos_])\n",
    "    print(tabulate(attribs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.401099Z",
     "start_time": "2019-11-08T11:44:06.397107Z"
    },
    "id": "0mUQ_Wnrl7rS"
   },
   "outputs": [],
   "source": [
    "print_sentences(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.410817Z",
     "start_time": "2019-11-08T11:44:06.404499Z"
    },
    "id": "PJ40jUMhl7rU"
   },
   "outputs": [],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KKtxIvyl7rV"
   },
   "source": [
    "## Matching \"zum Beispiel\"\n",
    "\n",
    "We are a bit frustrated, that the standard analysis pipeline does not know, that in German, \"z. B.\" is the abbreviation of \"zum Beispiel\" (like eg. is for \"for example\"), thus we would like to correct this.\n",
    "\n",
    "Our approach is to extend the pipeline and do a matching, whereby we replace the `lemma` form of \"z. B.\" to the appropriate long form.\n",
    "\n",
    "**IMPORTANT** design principle by SpaCy is, that one **always keeps the possibility to restore the original text**, so we are **NOT to modify `token.text`**. In the analysed form, we can do whatever we want.\n",
    "\n",
    "It is typical to add layers to the pipeline which modify the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNSIc9bMl7rW"
   },
   "source": [
    "For our purposes, we will use rule based matching to achieve our goals.\n",
    "\n",
    "A detailed description on rule based matching in SpaCy can be found [here](https://spacy.io/usage/rule-based-matching), or [here](https://medium.com/@ashiqgiga07/rule-based-matching-with-spacy-295b76ca2b68)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldkDPpOkl7rW"
   },
   "source": [
    "### Build the matcher\n",
    "\n",
    "With the help of rule based matching we create a matcher that reacts to the presence of \"z. B.\" exactly, then we use this matcher to define a pipeline step, that after matching, replaces the lemmas of the tokens \"z.\" and \"B.\" to  their full written equivalent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.417129Z",
     "start_time": "2019-11-08T11:44:06.412535Z"
    },
    "id": "uGj6hKLKl7rX"
   },
   "outputs": [],
   "source": [
    "zb_matcher = ... # Please instantiate a matcher with the appropriate parameters - think about all the words of the corpus...\n",
    ".... # Please add an appropriate pattern to the matcher to match \"z. B.\"\n",
    "\n",
    "\n",
    "\n",
    "@Language.component(\"zb_replacer\")\n",
    "def zb_replacer(doc):\n",
    "    matched_spans = []\n",
    "    # Please use the matcher to get matches!\n",
    "    matches = ....\n",
    "    # Plsease iterate over the matches!\n",
    "    for ....:\n",
    "        span = ... # get the span of text based on the matches coordinates!\n",
    "        matched_spans.append(span)\n",
    "        print(\"ZB MATCH!!!\")\n",
    "\n",
    "    # Please iterate over matched spans\n",
    "    for ....:  \n",
    "        # And replace their lemmas to the appropriate ones!\n",
    "        # Please observe, that you don't have the ID of the desired lemmas, just the their string form.\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd_i-IKfl7rX"
   },
   "source": [
    "### Register it to the pipeline\n",
    "\n",
    "After creating this processing step, we register it to be part of the pipeline and then run our analysis again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.421259Z",
     "start_time": "2019-11-08T11:44:06.418628Z"
    },
    "id": "7K1oVMKjl7rX"
   },
   "outputs": [],
   "source": [
    "# Plase register the new zb_replacer to the pipeline!\n",
    "# Think about, where to place it!\n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmYZiRsel7rY"
   },
   "source": [
    "### Re-do the analysis and observe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.442000Z",
     "start_time": "2019-11-08T11:44:06.422574Z"
    },
    "id": "Hh5oBA0fl7rY"
   },
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.448772Z",
     "start_time": "2019-11-08T11:44:06.443720Z"
    },
    "id": "Q_lvb2gkl7rZ"
   },
   "outputs": [],
   "source": [
    "print_tokens_for_sentence(doc,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcBUAPC6l7rZ"
   },
   "source": [
    "## What are those ugly pronunciation signs doing there?\n",
    "\n",
    "OK, so far so good. Let's observe, what is the problem with the first sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.473528Z",
     "start_time": "2019-11-08T11:44:06.450123Z"
    },
    "id": "ePiLpmall7ra"
   },
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.484081Z",
     "start_time": "2019-11-08T11:44:06.476136Z"
    },
    "id": "FO-eMTN8l7ra"
   },
   "outputs": [],
   "source": [
    "print_tokens_for_sentence(doc,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqpwzf-Yl7ra"
   },
   "source": [
    "As we can see, poor pipeline can not really cope with the pronunciation markings of the phonetic alphabet, and thus thinks, that the signs are representing a foreign proper noun. \n",
    "\n",
    "We would like to remedy this, and since we do expect further texts from the corpus to contain these inserted phonetics, we would like to match, merge and replace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6bnbo-el7ra"
   },
   "source": [
    "## Building up matcher for PRONUNCIATION\n",
    "\n",
    "To be more specific, we again first build up a matcher, that aims at the \"square brackets\" markings around the pronunciation. The task is to match everything between square brackets, or to be more specific: **everything that starts with an opening square bracket, and finishes with \";\"**.\n",
    "\n",
    "This matcher can then be used to:\n",
    "\n",
    "1. Merge the resulting matching `span` into one token\n",
    "2. Replace the token's lemma to \"PRONUNCIATION\"\n",
    "\n",
    "For this to be achievable, we have to first register \"PRONUNCIATION\" as part of the vocabulary, moreover mark it as [\"stopword\"](https://en.wikipedia.org/wiki/Stop_words). (More on SpaCy's stopword handling [here](https://medium.com/@makcedward/nlp-pipeline-stop-words-part-5-d6770df8a936)) See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.497568Z",
     "start_time": "2019-11-08T11:44:06.486489Z"
    },
    "id": "sUq8ae4Ml7rb"
   },
   "outputs": [],
   "source": [
    "# Please instantiate and build the matcher as before with the appropriate pattern!\n",
    "# Make it so, that the pattern will match ALL future pronunciations, not just the present one!\n",
    "....\n",
    "\n",
    "\n",
    "# We set the properties for the new word \"PRONUNCIATION\"\n",
    "lex = nlp.vocab['PRONUNCIATION']\n",
    "\n",
    "lex.is_stop = True\n",
    "\n",
    "@Language.component(\"pronunciation_replacer\")\n",
    "def pronunciation_replacer(doc):\n",
    "    \n",
    "    # Using the template above, please build a pronunciation replacer, that\n",
    "    # 1. Gets the matches\n",
    "    # 2. Iterates through them\n",
    "    # 3. Collects them into a list\n",
    "    # 4. initalizes - WITH context manager - a retokenizer\n",
    "    # 5. goes through the spans\n",
    "    # 6. merges them\n",
    "    # 7. sets the lemma of the merged span to the PRONUNCIATION lemma we created before\n",
    " \n",
    "\n",
    "\n",
    "nlp.add_pipe(\"pronunciation_replacer\", after=\"zb_replacer\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esKixpjjl7rb"
   },
   "source": [
    "### Observing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.623259Z",
     "start_time": "2019-11-08T11:44:06.500096Z"
    },
    "id": "-qnVljrtl7rb"
   },
   "outputs": [],
   "source": [
    "doc=nlp(text)\n",
    "print_tokens_for_sentence(doc,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXB46tVdl7rc"
   },
   "source": [
    "In the future, we decide, we would not want to include the pronunciation tokens in our view. So we have to mark them as wtopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmIdVtXMl7rc"
   },
   "source": [
    "### Registering PRONUNCIATION as a stopword\n",
    "\n",
    "Stopwords are typically those words, which do not contribute to the meaning of the sentence, are just there for syntactic reasons. There is a vague running list of these for languages. We will use and extend the German one in SpaCy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.627929Z",
     "start_time": "2019-11-08T11:44:06.625286Z"
    },
    "id": "18FTFl7Xl7rc"
   },
   "outputs": [],
   "source": [
    "# import stop words from GERMAN language data\n",
    "....\n",
    "\n",
    "# Add PRONUNCIATION to stopwords\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0T0Boayl7rc"
   },
   "source": [
    "But since we will only be able to manipulate the lemmas of the pronunciation markings, we would have to let SpaCy know, that - in contrast to the default behavior, where stopwords are filtered on `text` level, we would like to have a new property for words, that is based on `lemma` level stopword filtering.\n",
    "\n",
    "For these we will use extensions!\n",
    "\n",
    "For more info please see [here](https://spacy.io/api/token#set_extension)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.635721Z",
     "start_time": "2019-11-08T11:44:06.629472Z"
    },
    "id": "WNb_wMC4l7rc"
   },
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# Please define a function (or lambda expression!) that checks if a Token, or its lower case for, \n",
    "# OR it's lemma string is contained it he stopword list above.\n",
    "stop_words_getter ...\n",
    "\n",
    "# Set the above defined function as a extension for Token under the name \"is_lemma_stop\" as a getter!\n",
    "...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.672287Z",
     "start_time": "2019-11-08T11:44:06.637068Z"
    },
    "id": "ktcRz5Jxl7rd"
   },
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:06.677941Z",
     "start_time": "2019-11-08T11:44:06.673660Z"
    },
    "id": "XDFPJfbOl7rd"
   },
   "outputs": [],
   "source": [
    "print_tokens_for_sentence(doc,0, stopwords=True)\n",
    "\n",
    "assert len(list(doc.sents)[0]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2kKoL3Al7re"
   },
   "source": [
    "## Language detection\n",
    "\n",
    "We could also observe, that there is some English text inbetween our nice German sentences. We would like to detect foreign sentences and by later processing, ignore / skip them.\n",
    "\n",
    "For this to be achievable, we need some language detection capabilities.\n",
    "\n",
    "Luckily enough, we can make it part of our pipeline via [this extension](#https://spacy.io/universe/project/spacy-langdetect)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXj9fQQIl7re"
   },
   "source": [
    "### Standard installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.709209Z",
     "start_time": "2019-11-08T11:44:06.679245Z"
    },
    "id": "GnCLOnr0l7re"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install spacy-langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.732209Z",
     "start_time": "2019-11-08T11:44:07.714434Z"
    },
    "id": "Wa843sGal7rf"
   },
   "outputs": [],
   "source": [
    "#Please import the language detector!\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z0Af9ggl7rf"
   },
   "source": [
    "### Adding language detection to our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.736304Z",
     "start_time": "2019-11-08T11:44:07.733828Z"
    },
    "id": "fIRLLRN7l7rf"
   },
   "outputs": [],
   "source": [
    "# Please register it to the pipeline as the final step of processing!\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MErXqXncl7rf"
   },
   "source": [
    "### Observing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:07.776361Z",
     "start_time": "2019-11-08T11:44:07.737797Z"
    },
    "id": "hi8X9vdkl7rf"
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.097242Z",
     "start_time": "2019-11-08T11:44:07.780914Z"
    },
    "id": "4KcECbZul7rg"
   },
   "outputs": [],
   "source": [
    "attribs = []\n",
    "for sentence in doc.sents:\n",
    "    attribs.append([list(sentence)[:5],\"...\", sentence._.language])\n",
    "print(tabulate(attribs))\n",
    "\n",
    "# Please observe how one accesses anextension!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt3IX7kyl7rg"
   },
   "source": [
    "# Creating final generator for cleaned text\n",
    "\n",
    "Typically for a later stage of NLP, we would like to have a generator like function, which allows us to iteratively access the corpus, albeit in it's cleaned and encoded form. Integer encoding (as well as one hot encoding) are quite typical representations of text.\n",
    "\n",
    "In this spirit, we would like to implement a generator, that gives back an **array of lemmas OR lemma IDs for each sentence in the corpus, filtering out non-German sentences and punctuation / space marks**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.105282Z",
     "start_time": "2019-11-08T11:44:08.101443Z"
    },
    "id": "5vf7pv6jl7rg"
   },
   "outputs": [],
   "source": [
    "# Please implement a generator function that yields the text of the corpus as lists of sentences\n",
    "# Based on the parameters either as a list of strings or a list of IDs\n",
    "# It should filter out non-German sentences\n",
    "# as well as topwords based on lemmas\n",
    "# and punctuation and \"space like\" characters!\n",
    "\n",
    "def sentence_generator(doc, ids=False):\n",
    "    \n",
    "    ....\n",
    "   \n",
    "    yield out_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T11:44:08.144266Z",
     "start_time": "2019-11-08T11:44:08.106651Z"
    },
    "id": "rtEnUSeAl7rh"
   },
   "outputs": [],
   "source": [
    "for i in sentence_generator(doc):\n",
    "    print(i,\"\\n\")\n",
    "    \n",
    "for i in sentence_generator(doc, ids=True):\n",
    "    print(i,\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

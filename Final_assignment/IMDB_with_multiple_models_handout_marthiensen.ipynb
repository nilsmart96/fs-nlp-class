{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_wM75MwAoEn"
   },
   "source": [
    "# Sentiment classification - close to the state of the art\n",
    "\n",
    "The task of classifying sentiments of texts (for example movie or product reviews) has high practical significance in online marketing as well as financial prediction. This is a non-trivial task, since the concept of sentiment is not easily captured.\n",
    "\n",
    "For this assignment you have to use the larger [IMDB sentiment](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz) benchmark dataset from Stanford, an achieve close to state of the art results.\n",
    "\n",
    "The task is to try out multiple models in ascending complexity, namely:\n",
    "\n",
    "1. TFIDF + classical statistical model (eg. RandomForest)\n",
    "2. LSTM classification model\n",
    "3. LSTM model, where the embeddings are initialized with pre-trained word vectors\n",
    "4. fastText model\n",
    "5. BERT based model (you are advised to use a pre-trained one and finetune, since the resource consumption is considerable!)\n",
    "\n",
    "You should get over 90% validation accuracy (though nearly 94 is achievable).\n",
    "\n",
    "You are allowed to use any library or tool, though the Keras environment, and some wrappers on top (ie. Ktrain) make your life easier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QADQva7AoEq"
   },
   "source": [
    "__Groups__\n",
    "This assignment is to be completed individually, two weeks after the class has finished. For the precise deadline please see canvas.\n",
    "\n",
    "__Format of submission__\n",
    "You need to submit a pdf of your Google Collab notebooks.\n",
    "\n",
    "__Due date__\n",
    "Two weeks after the class has finished. For the precise deadline please see canvas.\n",
    "\n",
    "Grade distribution:\n",
    "1. TFIDF + classical statistical model (eg. RandomForest) (25% of the final grade)\n",
    "2. LSTM classification model (15% of the final grade)\n",
    "3. LSTM model, where the embeddings are initialized with pre-trained word vectors, e.g. fastText, GloVe etc. (15% of the final grade)\n",
    "4. fastText model (15% of the final grade)\n",
    "5. BERT based model (you are advised to use a pre-trained one and finetune it, since the resource consumption is considerable!) (20% of the final grade). For BERT you should get over 90% validation accuracy (though nearly 94% is achievable).\n",
    "6. Try out a more advanced LLM than pert and achieve a higher accuracy than BERT (10%)\n",
    "\n",
    "\n",
    "__For each of the models, the marks will be awarded according to the following three criteria__:\n",
    "\n",
    "(1) The (appropriately measured) accuracy of your prediction for the task. The more accurate the prediction is, the better. Note that you need to validate the predictive accuracy of your model on a hold-out of unseen data that the model has not been trained with.\n",
    "\n",
    "(2) How well you motivate the use of the model - what in this model's structure makes it suited for representing sentiment? After using the model for the task how well you evaluate the accuracy you got for each model and discuss the main advantages and disadvantages the model has in the particular modelling task. At best you take part of the modelling to support your arguments.\n",
    "\n",
    "(3) The consistency of your take-aways, i.e. what you have learned from your analyses. Also, analyze when the model is good and when and where it does not predict well.\n",
    "\n",
    "Please make sure that you comment with # on the separates steps of the code you have produced. For the verbal description and analyses plesae insert markdown cells.\n",
    "\n",
    "\n",
    "__Plagiarism__: The Frankfurt School does not accept any plagiarism. Data science is a collaborative exercise and you can discuss the research question with your classmates from other groups, if you like. You must not copy any code or text though. Plagiarism will be prosecuted and will result in a mark of 0 and you failing this class.\n",
    "\n",
    "After carefully reading this document and having had a look at the data you may still have questions. Please submit those question to the public Q&A board in canvas and we will answer each question, so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpCsyyH2AoEu"
   },
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "gEwjTzd8rqjR",
    "outputId": "d61a2181-0144-4029-9643-b891dc5ff821"
   },
   "outputs": [],
   "source": [
    "#!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xzf aclImdb_v1.tar.gz\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1yn618mAoFH"
   },
   "source": [
    "# Alternative with tf.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:12:20.884627Z",
     "start_time": "2020-10-11T12:12:17.747311Z"
    },
    "id": "UXx_Pj_7AoFJ"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-datasets > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T12:12:25.800798Z",
     "start_time": "2020-10-11T12:12:24.009095Z"
    },
    "id": "bhG1CnaSAoFU"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T13:25:00.555522Z",
     "start_time": "2020-10-11T13:22:43.089107Z"
    },
    "colab": {
     "referenced_widgets": [
      "56c09896c96d4baeb42b6d5f07958377",
      "45ac015941d44ba68e04b74bc81861c7",
      "",
      "65f1e0b0b08d45f5b0a3a5ebfdc4de2c",
      "e88c08fc881b48bca5bf9d3c87fb5890",
      "4947c18453e84356a4f6a99e05b62ce1"
     ]
    },
    "id": "oFk-Bh1xAoFc",
    "outputId": "e745adde-b8e8-4710-c70d-cb0dd7fd4fc5"
   },
   "outputs": [],
   "source": [
    "(ds_train,ds_test),ds_info = tfds.load(\n",
    "    name=\"imdb_reviews\",\n",
    "    split=[\"train\",\"test\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T13:26:27.027262Z",
     "start_time": "2020-10-11T13:26:27.020258Z"
    },
    "id": "apTjqy8VAoFo",
    "outputId": "ae203b4c-9f9b-4978-cca6-5571ade7bf77",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
       "    classification containing substantially more data than previous benchmark\n",
       "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
       "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_dir='/Users/nilsmart96/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# General text-labels definition for all models\n",
    "train_texts = [text.decode('utf-8') for text, label in tfds.as_numpy(ds_train)]\n",
    "train_labels = [label for text, label in tfds.as_numpy(ds_train)]\n",
    "test_texts = [text.decode('utf-8') for text, label in tfds.as_numpy(ds_test)]\n",
    "test_labels = [label for text, label in tfds.as_numpy(ds_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: TFIDF + classical statistical model (eg. RandomForest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83952\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84     12500\n",
      "           1       0.85      0.83      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# Create RandomForest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Build a Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),\n",
    "    ('clf', rf_classifier)\n",
    "])\n",
    "\n",
    "# Train the Model\n",
    "pipeline.fit(train_texts, train_labels)\n",
    "\n",
    "# Evaluate the Model\n",
    "predictions = pipeline.predict(test_texts)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model structure you've used for sentiment analysis, which combines TF-IDF representation with a RandomForest classifier, has some inherent characteristics that make it well-suited for this task. Let's discuss the key aspects:\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **TF-IDF Representation:**\n",
    "   - **Feature Selection:** TF-IDF helps in selecting the most informative words in the dataset by giving higher weights to words that are more specific to certain documents and less frequent across the entire dataset.\n",
    "   - **Sparse Representation:** The TF-IDF matrix is often sparse, meaning it has many zero entries. This can be beneficial in terms of memory efficiency and can lead to faster training and inference.\n",
    "\n",
    "2. **RandomForest Classifier:**\n",
    "   - **Ensemble Learning:** RandomForest is an ensemble learning method that builds multiple decision trees and merges their predictions. This helps to reduce overfitting and improves the generalization of the model.\n",
    "   - **Robust to Noisy Data:** RandomForest is robust to noisy data and outliers, making it suitable for handling real-world data with variations.\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - RandomForest models are relatively easy to interpret. You can analyze feature importances to understand which words contribute the most to the sentiment prediction.\n",
    "\n",
    "### Evaluation:\n",
    "\n",
    "1. **Accuracy:**\n",
    "   - The accuracy achieved (around 83.95%) is reasonably good for a binary sentiment classification task. It indicates that the model is making correct predictions on a large portion of the dataset.\n",
    "\n",
    "2. **Precision, Recall, and F1-Score:**\n",
    "   - Precision, recall, and F1-score are balanced for both positive and negative classes, indicating that the model performs well in terms of both identifying positive and negative sentiments.\n",
    "\n",
    "### Advantages and Disadvantages:\n",
    "\n",
    "1. **Advantages:**\n",
    "   - **Interpretability:** RandomForest models provide feature importances, allowing you to interpret which words are influential in sentiment prediction.\n",
    "   - **No Hyperparameter Tuning:** RandomForest often requires less hyperparameter tuning compared to other models, making it easier to use out of the box.\n",
    "\n",
    "2. **Disadvantages:**\n",
    "   - **Limited Context Understanding:** TF-IDF representation treats each word independently and doesn't capture the context between words. This limits the model's ability to understand the meaning of phrases or sentences.\n",
    "   - **Potential Overfitting:** While RandomForest is less prone to overfitting than individual decision trees, it can still overfit noisy data, and the model's complexity may lead to a loss of generalization on unseen data.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Experiment with More Advanced Embeddings:**\n",
    "   - Consider using word embeddings (e.g., Word2Vec, GloVe) or more advanced pre-trained language models (e.g., BERT, GPT) to capture richer semantic relationships between words.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - Experiment with hyperparameter tuning for the RandomForest model to see if further improvements can be achieved.\n",
    "\n",
    "3. **Ensemble Models:**\n",
    "   - Explore ensemble models that combine predictions from multiple models, potentially leveraging different types of features or representations.\n",
    "\n",
    "In summary, while the TF-IDF and RandomForest approach is a solid starting point, there is room for improvement by exploring more sophisticated representations and models to capture nuanced relationships within the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary additional libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Needed for both LSTM models\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Padding\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, truncating='post', padding='post')\n",
    "\n",
    "# Train/val split, so we can test with unseen data later\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_padded, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Rename the testing variables\n",
    "X_test = test_padded\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. LSTM classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 37s 56ms/step - loss: 0.5182 - accuracy: 0.7365 - val_loss: 0.3628 - val_accuracy: 0.8484\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 36s 57ms/step - loss: 0.2970 - accuracy: 0.8817 - val_loss: 0.3491 - val_accuracy: 0.8620\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.2201 - accuracy: 0.9194 - val_loss: 0.3808 - val_accuracy: 0.8608\n",
      "782/782 [==============================] - 13s 16ms/step\n",
      "Accuracy: 0.84184\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84     12500\n",
      "           1       0.83      0.86      0.84     12500\n",
      "\n",
      "    accuracy                           0.84     25000\n",
      "   macro avg       0.84      0.84      0.84     25000\n",
      "weighted avg       0.84      0.84      0.84     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's analyze the LSTM model's structure and evaluate its performance, discussing the advantages and disadvantages for sentiment analysis:\n",
    "\n",
    "### Model Structure:\n",
    "\n",
    "1. **Embedding Layer:**\n",
    "   - The Embedding layer learns a dense representation of words in a continuous vector space. This allows the model to capture semantic relationships between words and understand their contextual meaning.\n",
    "\n",
    "2. **Bidirectional LSTM Layer:**\n",
    "   - The Bidirectional LSTM layer processes input sequences in both forward and backward directions. This bidirectional processing helps capture long-range dependencies in the input data, making it well-suited for understanding the sequential nature of text.\n",
    "\n",
    "3. **Dense Output Layer:**\n",
    "   - The Dense layer with a sigmoid activation function is used for binary sentiment classification. It produces a probability score indicating the likelihood of a given input belonging to the positive class.\n",
    "\n",
    "### Evaluation:\n",
    "\n",
    "1. **Accuracy:**\n",
    "   - The LSTM model achieved an accuracy of approximately 84.36% on the test set, which is a notable improvement over the TF-IDF and RandomForest approach.\n",
    "\n",
    "2. **Precision, Recall, and F1-Score:**\n",
    "   - Precision, recall, and F1-score are balanced for both positive and negative classes, indicating that the model performs well in terms of both identifying positive and negative sentiments.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Contextual Understanding:**\n",
    "   - The LSTM model captures sequential dependencies in the data, allowing it to understand the context of words in a sentence. This is crucial for sentiment analysis where the meaning often depends on the arrangement of words.\n",
    "\n",
    "2. **Adaptability to Sequence Length:**\n",
    "   - LSTMs can handle variable-length sequences, making them suitable for tasks like sentiment analysis where the length of input texts may vary.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "1. **Computational Complexity:**\n",
    "   - LSTMs, especially bidirectional ones, can be computationally intensive and may require more resources during training and inference compared to simpler models.\n",
    "\n",
    "2. **Potential Overfitting:**\n",
    "   - Deep neural networks, including LSTMs, are susceptible to overfitting, especially when dealing with relatively small datasets. Regularization techniques may be necessary to address this issue.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Regularization:**\n",
    "   - Consider adding dropout layers to the model to reduce overfitting and improve generalization.\n",
    "\n",
    "2. **Hyperparameter Tuning:**\n",
    "   - Experiment with different hyperparameter settings, such as the number of LSTM units, embedding dimensions, and learning rates, to find optimal values for your specific task.\n",
    "\n",
    "3. **Ensemble Models:**\n",
    "   - Explore ensemble models that combine predictions from multiple models, potentially leveraging different architectures or representations.\n",
    "\n",
    "In summary, the LSTM model's ability to capture sequential dependencies and understand the context of words contributes to its improved performance in sentiment analysis compared to the TF-IDF and RandomForest approach. However, it comes with increased computational complexity and a potential risk of overfitting, which should be carefully addressed during model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 153s 239ms/step - loss: 0.5577 - accuracy: 0.7107 - val_loss: 0.4819 - val_accuracy: 0.7682\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 150s 240ms/step - loss: 0.4642 - accuracy: 0.7883 - val_loss: 0.4398 - val_accuracy: 0.8122\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 344s 551ms/step - loss: 0.3682 - accuracy: 0.8396 - val_loss: 0.3313 - val_accuracy: 0.8566\n",
      "782/782 [==============================] - 69s 87ms/step\n",
      "Accuracy: 0.8542\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86     12500\n",
      "           1       0.86      0.85      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model with pre-trained word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Create an embedding matrix with spaCy word vectors\n",
    "embedding_dim = nlp.vocab.vectors.shape[1]\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < max_words:\n",
    "        embedding_matrix[index] = nlp(word).vector\n",
    "\n",
    "# Build the LSTM model with pre-trained embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model with pre-trained word embeddings from spaCy demonstrates reasonable performance in sentiment analysis on the IMDb dataset. Let's discuss some aspects of the model and its evaluation:\n",
    "\n",
    "### Model Structure:\n",
    "1. **Embedding Layer with Pre-trained Word Vectors:**\n",
    "   - The model utilizes an embedding layer initialized with pre-trained word vectors from the spaCy model. This layer allows the model to capture semantic relationships between words based on their pre-existing contextual information.\n",
    "   \n",
    "2. **Bidirectional LSTM Layers:**\n",
    "   - The LSTM layers are bidirectional, allowing the model to consider both past and future context for each word. This is particularly useful in understanding the sentiment expressed in longer sequences.\n",
    "\n",
    "3. **Dense Layer with Sigmoid Activation:**\n",
    "   - The model ends with a dense layer with a sigmoid activation function, suitable for binary classification tasks like sentiment analysis.\n",
    "\n",
    "### Evaluation Results:\n",
    "- **Accuracy:** The model achieves an accuracy of approximately 83.8% on the test set, which is a reasonably good performance for sentiment analysis.\n",
    "\n",
    "- **Precision, Recall, and F1-Score:**\n",
    "  - Precision and recall values are balanced, indicating that the model is performing well for both positive and negative sentiment classes.\n",
    "  - The F1-score, which considers both precision and recall, is also balanced.\n",
    "\n",
    "### Advantages:\n",
    "1. **Effective Use of Pre-trained Embeddings:**\n",
    "   - Utilizing pre-trained word embeddings from spaCy leverages rich semantic information, enabling the model to understand the context and sentiment expressed in the reviews.\n",
    "\n",
    "2. **Bidirectional LSTM:**\n",
    "   - The bidirectional nature of the LSTM layers allows the model to capture dependencies in both directions, providing a more comprehensive understanding of the input sequence.\n",
    "\n",
    "3. **Reasonable Accuracy:**\n",
    "   - Achieving an accuracy of 83.8% is competitive, especially considering the complexity of sentiment analysis and the presence of nuanced language in movie reviews.\n",
    "\n",
    "### Disadvantages:\n",
    "1. **Fixed-Length Sequences:**\n",
    "   - The model uses fixed-length sequences, which may result in information loss for longer reviews. More sophisticated approaches, such as attention mechanisms, could be considered for handling variable-length sequences more effectively.\n",
    "\n",
    "2. **Computational Complexity:**\n",
    "   - The model's architecture, especially with bidirectional LSTM layers, can be computationally expensive and may require significant resources for training.\n",
    "\n",
    "3. **Potential for Overfitting:**\n",
    "   - Depending on the size of the dataset and the complexity of the model, there is a potential risk of overfitting. Regularization techniques or further model tuning might be explored to address this.\n",
    "\n",
    "In summary, the model demonstrates solid performance in sentiment analysis, leveraging pre-trained embeddings and bidirectional LSTMs. However, as with any modeling task, there are trade-offs, and further exploration and tuning could be done based on specific requirements and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  281132\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 4251333 lr:  0.000000 avg.loss:  0.223636 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.87564\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88     12500\n",
      "           1       0.88      0.87      0.87     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "import fasttext\n",
    "\n",
    "# Some modification to general labeling format from above\n",
    "train_labels_ft = [f'__label__{label}' for text, label in tfds.as_numpy(ds_train)]\n",
    "test_labels_ft = [f'__label__{label}' for text, label in tfds.as_numpy(ds_test)]\n",
    "\n",
    "# Save data to files as required by fastText\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, label in zip(train_texts, train_labels_ft):\n",
    "        f.write(f'{label} {text}\\n')\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, label in zip(test_texts, test_labels_ft):\n",
    "        f.write(f'{label} {text}\\n')\n",
    "\n",
    "# Train a supervised model\n",
    "model = fasttext.train_supervised(input='train.txt', epoch=10, lr=0.5)\n",
    "\n",
    "# Make predictions\n",
    "predictions = [model.predict(text)[0][0] for text in test_texts]\n",
    "\n",
    "# Convert predictions to binary labels\n",
    "binary_predictions = [int(label.split('__label__')[1]) for label in predictions]\n",
    "\n",
    "# Evaluate predictions\n",
    "accuracy = accuracy_score(y_test, binary_predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, binary_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FastText model you trained for sentiment analysis on the IMDb dataset has shown a good level of accuracy. Let's discuss what in the model's structure makes it suited for representing sentiment, evaluate its accuracy, and discuss its advantages and disadvantages:\n",
    "\n",
    "### Model Structure:\n",
    "\n",
    "FastText is a shallow neural network model designed for text classification. Key features of FastText that make it suitable for sentiment analysis:\n",
    "\n",
    "1. **Word Embeddings:**\n",
    "   - FastText uses word embeddings to represent words in the text. These embeddings capture semantic information about words, allowing the model to understand the context and meaning of words in the input.\n",
    "\n",
    "2. **n-gram Features:**\n",
    "   - FastText incorporates subword information through n-gram features. This helps in capturing morphological information and is especially useful for handling out-of-vocabulary words or variations of words.\n",
    "\n",
    "3. **Linear Classifier:**\n",
    "   - The model uses a linear classifier at the output layer for classifying the input into different sentiment classes. This simplicity allows for fast training and prediction.\n",
    "\n",
    "### Accuracy Evaluation:\n",
    "\n",
    "The accuracy achieved by the FastText model on the IMDb dataset is around 87.5%. This is a good accuracy level, indicating that the model is effective in distinguishing between positive and negative sentiments in movie reviews.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Efficiency:**\n",
    "   - FastText is known for its efficiency in training and prediction. It can handle large datasets and train models quickly.\n",
    "\n",
    "2. **Word Embeddings with Subword Information:**\n",
    "   - The combination of word embeddings and subword information helps the model handle unseen words and variations of words effectively.\n",
    "\n",
    "3. **Good Performance on Text Classification:**\n",
    "   - FastText is particularly effective for text classification tasks, including sentiment analysis. It performs well even with limited labeled data.\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "1. **Lack of Sequence Information:**\n",
    "   - FastText doesn't capture sequential information in the text. It treats the text as an unordered bag of words, which might limit its performance on tasks where word order is crucial.\n",
    "\n",
    "2. **Limited Context Understanding:**\n",
    "   - While word embeddings capture some context, they may not capture long-range dependencies or nuanced semantic relationships.\n",
    "\n",
    "3. **Not Suitable for Complex Tasks:**\n",
    "   - FastText is a simple model and may not perform as well on tasks that require a deep understanding of context, syntax, or intricate linguistic structures.\n",
    "\n",
    "In summary, FastText is a suitable choice for sentiment analysis tasks, especially when efficiency and speed are crucial. Its ability to handle subword information is advantageous, but it may lack the depth of understanding required for more complex natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     44\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 45\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Evaluation on the validation set\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# Tokenize and encode the testing data\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the validation set\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate predictions\n",
    "val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_true_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/wd/g0djm6xj5zng5hrgs2f9f1sw0000gn/T/ipykernel_7517/2780693155.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels)\n",
      "/var/folders/wd/g0djm6xj5zng5hrgs2f9f1sw0000gn/T/ipykernel_7517/2780693155.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_labels = torch.tensor(test_labels)\n",
      "Epoch 1/3:   2%|‚ñè         | 54/2500 [07:22<5:33:53,  8.19s/batch, loss=0.535]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Track correct predictions for accuracy calculation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels).clone().detach()\n",
    "\n",
    "# Tokenize and encode the testing data\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "test_labels = torch.tensor(test_labels).clone().detach()\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Use tqdm for the training loop\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as train_pbar:\n",
    "        for batch in train_pbar:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track correct predictions for accuracy calculation\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(predictions == labels).item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Update tqdm progress bar description\n",
    "            train_pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "    average_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Track correct predictions for accuracy calculation\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            val_correct_predictions += torch.sum(predictions == labels).item()\n",
    "            val_total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate validation accuracy and loss\n",
    "    val_accuracy = val_correct_predictions / val_total_samples\n",
    "    average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "          f\"Training Loss: {average_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Validation Loss: {average_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate predictions\n",
    "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_true_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592077aec04e4d2786377ff76d115e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb4cf5ec0fa48f3b27876d132cc15f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcba3184af574d32a0aa1b72fad260aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea0ec54c3d14460a2b47026b9787d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:710\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 814\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    817\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 940\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 879\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:813\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    811\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 813\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/contextlib.py:155\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/urllib3/response.py:715\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BaseSSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;66;03m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained RoBERTa model and tokenizer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroberta-base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Tokenize and encode the training data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_encodings \u001b[38;5;241m=\u001b[39m tokenizer(train_texts, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/transformers/modeling_utils.py:2778\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2764\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2766\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2767\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2776\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2777\u001b[0m     }\n\u001b[0;32m-> 2778\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2780\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2783\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/huggingface_hub/file_download.py:1431\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1429\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[0;32m-> 1431\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1441\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/huggingface_hub/file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    541\u001b[0m     displayed_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(‚Ä¶)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    544\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    545\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m    550\u001b[0m )\n\u001b[0;32m--> 551\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    553\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/envs/fs_base/lib/python3.11/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsSSLError(e)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "# Import necessary additional libraries\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Tokenize and encode the training data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# Tokenize and encode the testing data\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create DataLoader for training and testing data\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Set up optimizer and training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation on the validation set\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        val_predictions.extend(predictions.cpu().numpy())\n",
    "        val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluation on the test set\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        test_predictions.extend(predictions.cpu().numpy())\n",
    "        test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Evaluate predictions\n",
    "val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_true_labels, test_predictions))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMDB_with_multiple_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

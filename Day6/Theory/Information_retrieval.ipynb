{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HHlKK3Zi5re"
   },
   "source": [
    "# Motivation\n",
    "\n",
    "\n",
    "Matthew 7:7-11 *\"He who seeks finds\"*\n",
    "\n",
    "or finding \"the\" facts..\n",
    "\n",
    "Particularly important for LLMs as, they are stochastic models that do not have \"stored\" the correct \"facts\" but need to access them somewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwZRBrvzenND"
   },
   "source": [
    "# Information retrieval\n",
    "\n",
    "- Finding the content for a search question/query\n",
    "\n",
    "- In this context we focus on language based search\n",
    "\n",
    "\n",
    "Particularly relevant for large language models again, due to the limited lookback window\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYmODZxl12xv"
   },
   "source": [
    "## Classical era\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loPGdBZeFjw0"
   },
   "source": [
    "### Keyword matching, fuzzy matching\n",
    "\n",
    "**Keyword matching**\n",
    "- Tokenize text\n",
    "- Store location for every word\n",
    "- Reverse index, i.e. for every word store all locations\n",
    "- During search retrieval usally term frequency counts but TFIDF is also used (see below)\n",
    "\n",
    "E.g Apache Lucent search\n",
    "\n",
    "<a href=\"https://www.semanticscholar.org/paper/Apache-Lucene-4-Bia%C5%82ecki-Muir/2795d9d165607b5ad6d8b9718373b82e55f41606\"><img src=\"https://drive.google.com/uc?export=view&id=10cC0_zAMNXqO2ftGTN9qauiCGkJPcK3y\" width=60%></a>\n",
    "\n",
    "**Fuzzy matching:** returning queries with low edit distance (e.g. Levinshtein or Hemming distance)\n",
    "\n",
    "**Elastic Search:** distributed real time search based on Apache Lucent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp_j0wpx12vx"
   },
   "source": [
    "\n",
    "### Frequency based methods\n",
    "\n",
    "\n",
    "\n",
    "#### Bag of words:\n",
    "\n",
    "\n",
    "<a href=\"https://koushik1102.medium.com/nlp-bag-of-words-and-tf-idf-explained-fd1f49dce7c4\"><img src=\"https://drive.google.com/uc?export=view&id=1znqjOHSqDtIrqusgrdduPM_myg4WJHKI\" width=60%></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8U6ps1W12l_"
   },
   "source": [
    "####Term Frequency inverse document frequency (TF-IDF:\n",
    "\n",
    "- __not all words contribute equally to the meaning of a text__\n",
    "\n",
    "- words which are **generally frequent** in documents are less informative, than the ones that are **generally rare, but frequent in a small subset of documents**\n",
    "\n",
    "- INTUITION BEHIND TF-IDF: make rare words common in a subset of the document more prominent and effectively ignore common words.\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1600/1*8XpbsR4HdAHBXy5MgpIyug.png\"><img src=\"https://drive.google.com/uc?export=view&id=1l1ocR4yuVFJFvcWQahW-DJ0ZznOdnFUQ\" width=45%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://cdn-images-1.medium.com/max/1600/1*jNnpbGPxkjehlvTCXq9B8g.png\"><img src=\"https://drive.google.com/uc?export=view&id=17MPUwfV-oc785DRTzqONMMzj2o8pLy_e\" width=45%></a>\n",
    "\n",
    "- __logarithm__ turns __1 into 0__, and makes __large numbers__ (those much greater than 1) __smaller__. (More on this later.) Then a word that appears in __every single document__ will be effectively __zeroed out__, and a word that appears in very __few documents__ will have an even __larger count__ than before.\n",
    "\n",
    "- Term\n",
    "(Bit of recap from TF-IDF and co, then adding BM2.5, because it is still relevant and a strong baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGJC7FHr8apP"
   },
   "source": [
    "## Detour: Vector based search engine and comparing vectors\n",
    "\n",
    "- **Remember** whoever has can convert a search query into a vector and documents into vectors has a search engine\n",
    "\n",
    "- **By** finding the document vectors closest to the search vector\n",
    "\n",
    "## Basic querying: Cosine distance\n",
    "\n",
    "<a href=\"https://slideplayer.com/slide/5993529/20/images/16/Documents+%26+Query+in+n-dimensional+Space.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1Yi4FdZRy8cUqdAfSYpOyz0-ZNEPQKeEa\" width=45%></a>\n",
    "\n",
    "<a href=\"http://blog.christianperone.com/wp-content/uploads/2013/09/Dot_Product.png\"><img src=\"https://drive.google.com/uc?export=view&id=1ja1csLYW2zj8_YoPBqSaZQDGj5QizQ5I\" width=25%></a>\n",
    "\n",
    "## Alternative distance metrics\n",
    "\n",
    "### Euclidean distance\n",
    "\n",
    "<a href=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Euclidean_distance_3d_2_cropped.png/1024px-Euclidean_distance_3d_2_cropped.png\"><img src=\"https://drive.google.com/uc?export=view&id=1QUWzWVynns5SvAXm3hhH38_Zl9v5T5SW\" width=35%></a>\n",
    "\n",
    "In case of frequency based semantic vectors we would naively be tempted to go for the full Euclidean distance calculations, but that would __disregard__ the fact, that in case of a query and a document, __radically different frequencies will be present__ in case of words, so we might be better served with the cosine approach, which focuses on relative differences in meaning regardless of frequency.\n",
    "\n",
    "\n",
    "### Earth movers's (Mahalanobis) distance\n",
    "\n",
    "\n",
    "<a href=\"http://drive.google.com/uc?export=view&id=1L1jRYCjfRbQjGyqVKssLB2pfOc47Mch7\"><img src=\"https://drive.google.com/uc?export=view&id=1bEkjN-KOAmlyNU9zw5j_btJLAuoGIjvx\" width=40%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://slideplayer.com/slide/4511821/15/images/30/Option+3%3A+The+Earth+Mover+Distance+%28EMD%29.jpg\"><img src=\"https://drive.google.com/uc?export=view&id=1bcKY4nEiu7BCMdtrS3yEXVvQz-jO1Rna\" width=45%></a>\n",
    "\n",
    "<a href=\"https://vene.ro/images/wmd-obama.png\"><img src=\"https://drive.google.com/uc?export=view&id=1Oqqr8Gr_Qh2l0YROpjE6Ri3WKkxdiwsC\" width=45%></a>\n",
    "\n",
    "The usage of EMD makes sense in this context, since we suppose again, that certain locations of our vectors are representing some meaningful units, thus some kind of \"topic mapping\" is going on in case of a distance measurement.\n",
    "\n",
    "For the usage of EMD in semantic spaces see [this](http://proceedings.mlr.press/v37/kusnerb15.pdf) article.\n",
    "\n",
    "**Fair warning:** The __runtime__ requirements of EMD are __non linear__. For production environments this can mean a lot of CPU load. (Though [Radim Řehůřek](https://radimrehurek.com/about/) in [Gensim](https://radimrehurek.com/gensim/) [wmdistance](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.wmdistance.html) did some work on speeding it up. For easy usage see [here](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html))\n",
    "\n",
    "\n",
    "\n",
    "For a more general discussion about the different distance metrics in context of semantic clustering see [this](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf) article.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiMKENT0-_fM"
   },
   "source": [
    "#### Approximate nearest neighbor Search\n",
    "Nearest neighbour search - find the closest matching vector (of a document) based on the vector of the search query (e.g. using cosine similarity.\n",
    "\n",
    "Problem: NP-hard - requires to compare the search query vector with every document vector\n",
    "\n",
    "**Approximate nearest neighbour search**\n",
    "\n",
    "General idea\n",
    "- During indexing chunk similar indices together (e.g. documents with similar vectors)\n",
    "- During search do a matching on a \"higher level aggregte index\" first, only then go into matching with each individual entry\n",
    "\n",
    "E.g  \n",
    "\n",
    "KD-Tree\n",
    "The K-Dimensional Tree recursively divides the space into hyperrectangles along various dimensions at each level.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://www.researchgate.net/figure/The-example-of-the-KD-tree-Algorithms-2D-3D_fig3_263937521\"><img src=\"https://drive.google.com/uc?export=view&id=1Yn9J_TfizB6Fns7HWIk3skUeknE9tUN_\" width=60%></a>\n",
    "\n",
    "\n",
    "see also [here](https://medium.com/@brijesh_soni/the-potential-of-approximate-nearest-neighbors-ann-in-high-dimensional-spaces-579567e4f1a7) for a mode detailed description\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJGTSXqfPYz5"
   },
   "source": [
    "## Neural era\n",
    "\n",
    "### Word embeddings for search\n",
    "\n",
    "- We already saw how word embeddings work through either GloVe or Word2vec\n",
    "- Each word is mapped to a unique word vector, that represent's its meaning\n",
    "- Where similar words lie together in space\n",
    "- The difficulty in the context of search is that each document is composed of multiple (more than one) words, i.e. how should word vectors be aggregated?\n",
    "\n",
    "**Solutions**\n",
    "\n",
    "**Averaging**\n",
    "Simply average over the word vectors in a document\n",
    "\n",
    "But we already know that some words are more informative than others...\n",
    "\n",
    "It certainly makes sense to at least remove uninformative words such as stop-words\n",
    "\n",
    "**TF-IDF**\n",
    "Weigh the word vectors according to their TF-IDF score when aggregating them\n",
    "\n",
    "**POS**\n",
    "Use part of speech tags, e.g. only look for nouns\n",
    "\n",
    "**[SIF](https://oar.princeton.edu/bitstream/88435/pr1rk2k/1/BaselineSentenceEmbedding.pdf)**\n",
    "Simple but effective baseline:\n",
    "sim\n",
    "\n",
    "\n",
    "\n",
    "- most word embedding\n",
    "methods, since they seek to capture word cooccurence probabilities using vector inner product, end\n",
    "up giving large vectors to frequent words\n",
    "\n",
    "Do two things:\n",
    "(1) Reduce the weight of frequently occuring terms\n",
    "(2) Do a PCS on the word vectors and remove the first component as this is usually dominated by highly frequently but uninformative words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmuukxfTQFFB"
   },
   "source": [
    "### \"Thought vectors\"\n",
    "\n",
    "- Genearted vector of text by passing through LSTM encoder and taking out the hidden state or cell state fter the encoder\n",
    "\n",
    "<a href=\"https://adityaroc.medium.com/importance-of-thought-vector-in-seq2seq-model-407f1abb4da4\"><img src=\"https://drive.google.com/uc?export=view&id=1IpilOsmQ7nwUuvpX2U30AGJrhAUBgtwq\" width=60%></a>\n",
    "\n",
    "\n",
    "the typical search problem - throwing in a coupl of words as a query and expecting the the right paragraph/ answer is particularly large here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2BjzobfWNBv"
   },
   "source": [
    "### The age Transformers\n",
    "\n",
    "- Remember the **quadratic complexity scaling of the attention mechanism** -> you can't just throw in text beyond the input window (which will always be limited to a certain number of tokens (e.g. 5000) or have some sparse attention mechanism behind it\n",
    "- Chunking of the text and a search engine on top of this is needed to find the relevantant parts for any longer text\n",
    "\n",
    "\n",
    "- If there is only an embedding for every token as an output need to go back to averaging methods again. But Fortuantely..\n",
    "\n",
    "- A lot of large language models have **sentence transformer versions** where in one form or another the next sentence or parts of it need to be predicted from a [sentence vector](https://github.com/UKPLab/sentence-transformers)\n",
    "- An original example is BERT's CLS token\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8\"><img src=\"https://drive.google.com/uc?export=view&id=1tYMp0-k1dyJOLeaxcjbZ56D8BjwSOXwm\" width=60%></a>\n",
    "\n",
    "\n",
    "Below are some tutotrials on how to use large language models to build a search engine:\n",
    "- [here](https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8)\n",
    "- [here](https://neptune.ai/blog/building-search-engine-with-pre-trained-transformers-guide)\n",
    "\n",
    "\n",
    "### Recent advancements\n",
    "\n",
    "- Dedicated specialist embedding models\n",
    "\n",
    "- [Instructor](https://huggingface.co/hkunlp/instructor-xl), an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning.\n",
    "\n",
    "See also\n",
    "[here](https://huggingface.co/blog/mteb)\n",
    "[here](https://github.com/xlang-ai/instructor-embedding)\n",
    "\n",
    "\n",
    "**MTEB Dataset for evaluation - Massive Text Embedding Benchmark**\n",
    "- Spans 8 embedding tasks covering a total of 58 datasets and 112 languages.\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2210.07316.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1wCDDaHxl9UL1VgwrqIyeFatlzwcNd9lZ\" width=60%></a>\n",
    "\n",
    "- Through the benchmarking of 33 models on\n",
    "MTEB, we establish the most comprehensive\n",
    "benchmark of text embeddings to date. We\n",
    "find that no particular text embedding method\n",
    "dominates across all tasks.\n",
    "\n",
    "\n",
    "- The current lead-board can be found here: [here](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "\n",
    "- Especially interesting: Instructor-XL, as a prompt based embedder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CumiW3xGPfPn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNePsY3Rs/EVztIHpAqY921",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

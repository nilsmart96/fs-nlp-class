{"cells":[{"cell_type":"markdown","id":"9ee91621","metadata":{"id":"9ee91621"},"source":["# How are tasks solved in nlp - a short history\n","\n","## Motivation: the many tasks of NLP\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1kZ0f5h26BkTVNZBCcO1WNqE1qi10od_Q\" width=55%>\n","\n","\n","[source](https://medium.com/nlplanet/two-minutes-nlp-33-important-nlp-tasks-explained-31e2caad2b1b)\n","\n","NLP as a field is pretty wide in a sense, since __multiple tasks__ (suprvised or unsupervised) can be endeavoured to be solved based on the __same text__ and - importantly - it's __representation__ in a way that is conducive to machine learning. We saw ample examples for this so far."]},{"cell_type":"markdown","id":"edb6fb1e","metadata":{"id":"edb6fb1e"},"source":["## Historical context - A general shift in modeling approaches"]},{"cell_type":"markdown","id":"5ecd3dff","metadata":{"id":"5ecd3dff"},"source":["## Broader context\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1rzWcpZvWDfCIltJGh1P6tN4uaW_eFfoj\" width=80%>\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1lJC66UW0YvaTNM8ZRqs86ntAQid6Y8Ar\" width=80%>\n"]},{"cell_type":"markdown","id":"bc14e708","metadata":{"id":"bc14e708"},"source":["## \"Paradigms\" - Stages of development in NLP\n","\n","If we focus our attention on the development of NLP (and completely disregard the rule based or \"Ontological\" approaches of [\"Good old-fashioned AI\"](https://en.wikipedia.org/wiki/GOFAI), we can distinguish clear paradigms of model training and problem solving in NLP, that followed each-other in rapid (actually accelerating) succession."]},{"cell_type":"markdown","id":"b03706cb","metadata":{"id":"b03706cb"},"source":["### Non-learned feature extraction + custom model"]},{"cell_type":"markdown","id":"000754dd","metadata":{"id":"000754dd"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1NtZy--0eGGg6S2h6D8hFndv3ko8yPiYb\" width=35%>\n","\n","\n","[source](https://www.semanticscholar.org/paper/A-novel-text-mining-approach-based-on-TF-IDF-and-Dadgar-Araghi/ded138171f35309c0af9ecc98eeffb90f0e8f993)\n","\n","(As late as 2016, surprisingly!)\n","\n","In the now \"classical\" paradigm, the features from textual documents were extracted by an elaborate pipeline of hand crafted transformations, that resulted in the representation of texts in terms of word frequency properties. This representation was then used to train a specific classifier for a given task in a supervised manner."]},{"cell_type":"markdown","id":"fd9acc20","metadata":{"id":"fd9acc20"},"source":["### Learned features + custom models \"on top\"\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1V9w8DwNLQ0w_uLvYCnjl7d9TJjNAzsak\" width=20%>\n","\n","[source](https://www.semanticscholar.org/paper/Word2Vec-model-for-sentiment-analysis-of-product-in-Fauzi/ba511aa8390e4a48e4a8273d9e24c35bcfe4cd96)\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=17h6urUe2IySvvQd-DyoValRYjy5dwB_k\" width=55%>\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1X2HeILaqtd-dAxrhcRYtlTtMrliiPJPx\" width=70%>\n","\n","[Distributed Representations of Sentences and Documents](https://arxiv.org/abs/1405.4053)\n","\n","The big change happened with the introduction of unsupervised learning based representations, namely [word2vec](https://en.wikipedia.org/wiki/Word2vec) in 2013-14. The main paradigm change was, that an unsupervised predictive training task was found to be extremely efficient in coming up with high quality word representations, that could with simple (like averaging) or more complicated (like [SIF](https://openreview.net/pdf?id=SyK00v5xx)) approaches be utilized as representations for texts, and so, custom models could be trained using them as inputs."]},{"cell_type":"markdown","id":"5c6553e4","metadata":{"id":"5c6553e4"},"source":["### End-to-end learned models\n","\n","\n","The next level of complexity - and with it performance - arrived by using pre-trained \"embeddings\" (mainly word2vec) as part of and end-to-end pipeline, where every component was neural, mainly in form of LSTMs or their more complex \"seq2seq\" variants.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1cftnuhcP9ZhwsGRo75qN5x2kNHsaVvmw\" width=55%>\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1qXVsSWsny_UbBxbwtvrpv51dnNbwPG6x\" width=55%>\n","\n","[source](https://core.ac.uk/download/pdf/226439962.pdf)\n","\n","Here, the pre-trained layer of word2vec only represented a useful input transformation, the heavy lifting was still done by the task specificly trained architecture on top of them."]},{"cell_type":"markdown","id":"59cc9c7f","metadata":{"id":"59cc9c7f"},"source":["### Pretrain and finetune paradigm\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1kNI6DrMJNEMV0rZogmClIQ7187OhWX9w\" width=35%>\n","\n","\n","[Source](https://arxiv.org/abs/2109.01652)\n","\n","Though this paradigm reached fame with the advent of [transformer models](https://arxiv.org/abs/1706.03762), especially [BERT](https://arxiv.org/abs/1810.04805), the big leap in performance in the pretrain-finetune paradigm may be attributable more to the paper about [ULMFiT - Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146).\n","\n","The paradigm centers on __pretrained deep models__, that get applied to task specific settings by __finetuning__.\n","\n","\n","The breakthrough came, as the __\"gradual unfreezing\"__ method prevented the destruction of the knowledge learned by the neural models during the __\"pre-training\"__ process, so a task specific __fine-tuning__ was deemed to be very successful.\n","\n","<a href=\"https://humboldt-wi.github.io/blog/img/seminar/group4_ULMFiT/Figure_21.png\"><img src=\"https://drive.google.com/uc?export=view&id=1KxNr1UqL_1q7FTNjLv8SXivkuFKB343H\" width=65%></a>\n","\n","#### Cut and/or add a layer\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1SW3xfa1FYQ1TGXaorJHmcx-wAJpJPj4g\" width=55%>\n","\n","In this paradigm the main method for task customization was to either replace the last \"task\" layer of the network, or add a \"task specific\" layer as a new output."]},{"cell_type":"markdown","id":"02bea613","metadata":{"id":"02bea613"},"source":["#### \"Sidenote\": Transformers\n","\n","\n","With the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) a new, extremely important architecture, the \"transformer\" emerged.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1SoM5ha5ZikSd3jRxdpZPCNB4xidhh8Tr\" width=65%>\n","\n","The main advantages of this architecture were:\n","- Parallelizability on GPU (circumventing the LSTM's bottlenecks)\n","- Excellent utilization of information in long context windows (LSTM's effective window size was always debated)\n","\n","This allowed for the __rapid scaling up of model depth__, and combined with the pretrain and finetune paradigm established the dominance of deep pretrained models."]},{"cell_type":"markdown","id":"b6704d04","metadata":{"id":"b6704d04"},"source":["##### \"More sidenote\": quadratic complexity\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1Uafn-mvfStrmhcpHuAyCwAFYoLnHKyi-\" width=65%>\n","\n","Though some serious problems exist in transformers, namely [quadratic computation complexity](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/custom/15839671.pdf), there are methods that attempt to scale transformers to extremely long context sizes, like [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860), [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) or more recently [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062).\n","\n","Suffice it to say, that a kind of \"cottage industry\" sprang up with respect to transformer verisons.\n","\n","For a nice genealogy, see:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1TnQFZVz5zdOiPF6wN1hzyDYenjc-nHbQ\" width=65%>\n","\n","Source: [The Practical Guides for Large Language Models ](https://github.com/Mooler0410/LLMsPracticalGuide)"]},{"cell_type":"markdown","id":"611282c8","metadata":{"id":"611282c8"},"source":["#### \"Adapter\" finetuning\n","\n","As model sizes became prohibitively large to train on consumer grade hardware, the - in a sense a kind of \"reaching back to old style\" - idea came to __\"freeze\" the pretrained model__, and only take a shallow network to finetune.\n","\n","This shallow network was in the beginning a new, full \"task specific\" output layer, but then more efficient methods like \"PEFT\" or [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf) came up with __adatpter layers__, that are built in, task specificly \"after the fact\" finetuneable layers, we can inject to the model \"cheaply\"."]},{"cell_type":"markdown","id":"fcb5ed8c","metadata":{"id":"fcb5ed8c"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1W1XqUMl9Zud7pRldI6qPKkI2cfKvQ_kG\" width=65%>"]},{"cell_type":"markdown","id":"ef105b5e","metadata":{"id":"ef105b5e"},"source":["#### LoRA: [Lower-Rank Adaptation](https://arxiv.org/abs/2106.09685)\n","<img src=\"http://drive.google.com/uc?export=view&id=1qWN9DVYkhyQ1c3lrbiXqyizaw3kMeg4_\" width=35%>\n","\n","A more modern approach to this is [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n","\n","\n","\n"," Inspired by this, we hypothesize the updates to the weights also have a low \"intrinsic rank\" during adaptation. For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, we constrain its update by representing the latter with a low-rank decomposition $W_0+\\Delta W=W_0+B A$, where $B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$, and the rank $r \\ll \\min (d, k)$. During training, $W_0$ is frozen and does not receive gradient updates, while $A$ and $B$ contain trainable parameters. Note both $W_0$ and $\\Delta W=B A$ are multiplied with the same input, and their respective output vectors are summed coordinate-wise. For $h=W_0 x$, our modified forward pass yields:\n","$$\n","h=W_0 x+\\Delta W x=W_0 x+B A x\n","$$\n","\n","\n","Adapting the typical LLM loss\n","\n","<a href=\"https://arxiv.org/abs/2106.09685\"><img src=\"https://drive.google.com/uc?export=view&id=1vOjv-whu3HIXv0lIu5bGM4gNTNSJH40a\" width=60%></a>\n","\n","\n","\n","\n","Performance metrics\n","\n","<a href=\"https://arxiv.org/pdf/2001.08361.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1KB3WeD0vTjV32VUMrFfdh6TN4L1GO7iY\" width=60%></a>\n","\n","\n","More detailed explanation can be found [here](https://xiaosean5408.medium.com/fine-tuning-llms-made-easy-with-lora-and-generative-ai-stable-diffusion-lora-39ff27480fda), or an exen more excellent intro [here](https://lightning.ai/pages/community/tutorial/lora-llm/).\n"]},{"cell_type":"markdown","source":["**[QLoRA](https://arxiv.org/pdf/2305.14314.pdf) even further reduces memory and compute requirements**\n","\n","We present QLORA, an efficient finetuning approach that reduces memory usage enough to **finetune a 65B parameter model on a single 48GB GPU** while\n","**preserving full 16-bit finetuning task performance**. QLORA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank\n","Adapters (LoRA). Our best model family, which we name Guanaco, outperforms\n","all previous openly released models on the Vicuna benchmark, reaching 99.3%\n","of the **performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU**.\n","\n","Improvements:\n","\n","- **Double Quantization**: A method that reduces the memory footprint by quantizing the quantization constants, saving about 0.37 bits per parameter.\n","\n","- **Paged Optimizers**: A technique that uses NVIDIA unified memory to avoid memory spikes during gradient checkpointing."],"metadata":{"id":"AkOjKy5wsruB"},"id":"AkOjKy5wsruB"},{"cell_type":"markdown","id":"da909dc2","metadata":{"id":"da909dc2"},"source":["### Zero shot learning, \"prompting\""]},{"cell_type":"markdown","id":"5636c0f3","metadata":{"id":"5636c0f3"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1YoWhTN3bW9mrpjTH7NHm0S_c69M6__Da\" width=35%>\n","\n","\n","[Source](https://arxiv.org/abs/2109.01652)"]},{"cell_type":"markdown","id":"03ab559f","metadata":{"id":"03ab559f"},"source":["Quite quickly after the emergence of large pre-trained models, it was realized, that the models were capable of __non-trivial performance on unseen tasks without any training__. Subsequently, many papers (like [this](https://arxiv.org/pdf/1909.00161.pdf)) started to investigate the phenomena, which revived the fields of [zero shot learning](https://en.wikipedia.org/wiki/Zero-shot_learning) and [few-shot learning](https://en.wikipedia.org/wiki/Few-shot_learning) that were kind of nieche disciplines up till that point."]},{"cell_type":"markdown","id":"6022a518","metadata":{"id":"6022a518"},"source":["In contrast to previous approaches, where examples were used to modify weights — essentially training the networks — in the 'zero shot' learning paradigm, weights **remain unchanged**.\n","\n","In this paradigm the models already went enough __general knowledge__ in their weights (which are completely frozen), so that they can __output texts that can be taken as answers__.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1KZtdSCh4uZ-45ygY4S7ctSHOMwcaar6y\" width=65%>\n","\n","\n","In their paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) the authors establish the idea, that __the main form of interaction with a model is via it's textual inputs and outputs__, that can represent a wide variety of tasks.\n","\n","Since the performance of certain large models in a zero shot context beat the finetuned state-of-the-art, the paragidm of __in context learning__, a form of zero shot learning became dominant.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1huLGkkIx4wwzaqGP7IOPh6wW_OwkGOrN\" width=55%>\n","\n","\n","Source: \"GPT3\" - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n","\n","The main observation later on will be that __the instructions in the context__, that is __the \"prompts\"__ will have a very strong influence on the task specific performance!\n","\n","It is also important to note, that __prompt based \"learning\" has to be strongly distinguished from finetuning__!!! In \"in context learning\" the model weights are completely frozen, so it is kind of a misnomer (albeit extremely popular) to call this paradigm learning at all.\n","\n","Prompting is a way to get a model to execute a task. Learning in the gradient sense is not involed.\n","\n"]},{"cell_type":"markdown","id":"542cd833","metadata":{"id":"542cd833"},"source":["#### Instruction finetuning\n","\n","Further observation was, that if large [\"foundational models\"](https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models) models were explicitly finetuned after their initial unsupervised pre-training in a next phase __to follow instructions__ (like in the line of [\"InstructGPT\"](https://arxiv.org/abs/2203.02155)), their performance in solving zero shot tasks in the \"in context learning\" sense (so by responding to prompts) becomes even more impressive.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1HCOV0jyp265sSOFsiKmQjypMWtPU-w5U\" width=35%>\n","\n","\n","FLAN stands for Frozen Language Model Adaptation Networks\n","\n","Source: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)"]},{"cell_type":"markdown","id":"c174c8a3","metadata":{"id":"c174c8a3"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=16ApApmggJQZ9xzq4DdcCx1ht8o67Gc25\" width=65%>\n","\n","\n","Source: [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)\n","\n","Thus, the era of \"instruction finetuned pre-trained Large Language Models\" was born."]},{"cell_type":"markdown","id":"5vaOPrFb9k-_","metadata":{"id":"5vaOPrFb9k-_"},"source":["#### Reinforcement learning from human feedback (RLHF)\n","\n","An additional method, building on the results of instruction finetuning is the incorporation of (sparse) human feedback and preferences via reinforcement learning.\n","\n","\n","\n","[Learning to summarize from human feedback\n","](https://arxiv.org/abs/2009.01325)"]},{"cell_type":"markdown","id":"LY4SCOIxgE3Y","metadata":{"id":"LY4SCOIxgE3Y"},"source":["##### Motivation\n","- Large language models (LLMs) excellent in producing coherent and consistent text\n","- However, text may often not be exactly what human is expecting e.g. from a question\n","- Fine tune model for specific tasks such as text summarization\n","- Thus this technique requires a pre-trained large language model!\n","\n"]},{"cell_type":"markdown","id":"qKiRnMfLgokh","metadata":{"id":"qKiRnMfLgokh"},"source":["##### Particular (original) setting - **text summarization**"]},{"cell_type":"markdown","id":"acCi0Jlyg825","metadata":{"id":"acCi0Jlyg825"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1bJEFGUSo5duA341srpxbuOmdpC9Cm7Sd\" width=600 heigth=600>\n","\n","\n"]},{"cell_type":"markdown","id":"b5iNHpKbhUGO","metadata":{"id":"b5iNHpKbhUGO"},"source":["##### Approach\n"]},{"cell_type":"markdown","id":"bM-VPJSMhlGl","metadata":{"id":"bM-VPJSMhlGl"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1QGN2SyvnGZPz3F3WPvkd3IwsklvDjDJZ\" width=1000 heigth=1000>\n","\n"]},{"cell_type":"markdown","id":"UHD_-C5Uil9T","metadata":{"id":"UHD_-C5Uil9T"},"source":["**RL part in more detail**\n","- [PPO algorithm](https://openai.com/research/openai-baselines-ppo)\n","- Specific version of actor critique method\n","- Each time step BPE (Byte Pair Encoding)\n","\n","- Full reward $R$ can be written as:\n","$$\n","R(x, y)=r_\\theta(x, y)-\\beta \\log \\left[\\pi_\\phi^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid x)\\right]\n","$$\n","\n","- Second term penalizes the KL divergence between the learned RL policy $\\pi_\\phi^{\\mathrm{RL}}$ with parameters $\\phi$ and this original supervised model $\\pi^{\\mathrm{SFT}}$\n","- Additional penalty super important as RL model might otherwise find nonsensical results that get high reward from the classifier (in a sense adversarial examples)\n","- Outputs not too different from what the model produces\n"]},{"cell_type":"markdown","id":"loH_YonolFK8","metadata":{"id":"loH_YonolFK8"},"source":["##### Transfer Learning Task\n","- Same pre-trained algorithm applied to summarization of news articles\n","- Significantly outperforms models trained on supervised basline"]},{"cell_type":"markdown","id":"wuHmyfZymSs-","metadata":{"id":"wuHmyfZymSs-"},"source":["##### Performance comparison\n"]},{"cell_type":"markdown","id":"tTg_iACymWpe","metadata":{"id":"tTg_iACymWpe"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=13ZkkZOYAE5zy0PzJr6HlLVU7cvC4xCaa\" width=80%>\n","\n","\n"]},{"cell_type":"markdown","id":"b_bBGfbzpT7E","metadata":{"id":"b_bBGfbzpT7E"},"source":["**Note that the paper does not tell us a lot of important details**\n","- What parts of the model are actually retrained\n","- How long the retraining is done for\n","- How exactly the original language model is used (most likely it provides an input for the RL algorithm)"]},{"cell_type":"markdown","id":"8824c036","metadata":{"id":"8824c036"},"source":["#### Sidenote: on the openness of AI\n","\n","As the instruction finetuning results show, good quality (instruction) data is still important, publication of finetune procedures and datasets would be key, and in this regard, the leading research group at OpenAI is not exactly transparent, and protects it's business interests.\n","\n","Thus, such initiatives as [\"The Pile\"](https://pile.eleuther.ai/) as a pretraing dataset, [OpenAssistant's dataset](https://github.com/LAION-AI/Open-Assistant/blob/main/docs/docs/data/datasets.md) for instruction finetuning, open for research models like [LLaMA]() or fully open source models like [Pythia](https://github.com/EleutherAI/pythia) are of vital importance."]},{"cell_type":"markdown","id":"1bb4686e","metadata":{"id":"1bb4686e"},"source":["# Works, but why?\n","\n","Since the \"zero shot\" paradigm represents quite a strong change from what used to be the standard for machine learning, some more investigation of the mechanisms behind it can be useful."]},{"cell_type":"markdown","id":"bee7ace8","metadata":{"id":"bee7ace8"},"source":["## What's in the box? - Investigation of structural properties of transformers\n","\n","\n","### What behavior do the (pre- and instruction) trainings  emphasize?\n","\n","It is important to understand, that already the pre-training of language models - since it is done on an extremely wide variety of documents - emphasizes contextuality, that is, a model can not hope to solve a task sufficietly well, if it does not influence it's generation to adhere to local contexts.\n","\n","The explicit instruction finetuning approaches, (such as [this](https://arxiv.org/abs/2203.02155)) give even more emphasis to context, since in case of dialogues for example, people explicitly prefer in context consistent behavior (like \"remembering\" past conversation steps) for the model, thus give __extremely strong signal to enhance in context generation__.\n","\n","We can see this as a strong push towards enhancing the abilities of models in __in context learning__, that is, their __few and zero shot performance__.\n","\n","### In context learning and \"induction heads\"\n","\n","Recently, some in-depth analysis was carried out that identified (amongst others) one important mechanism that modern Large Language Models based on the transformer architecture learn to solve their tasks. The researchers gave the name __\"induction heads\"__ to the attention patterns that were most frequent.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1InObwz_wR9ry32lDLPHeS72eySeoJYhj\" width=65%>\n","\n","The basic induction head is learning a bigram like model of \"if this then that\" probability association between a word and the one that most frequently follows it. This enhances the ability of the model to utilize bigram re-occurence __in context__ to give nice baseline predictions.\n","\n","Over and beyond these simple heads more sophisticated complex \"induction\" mechaisms are learned by the model (given it has at least 2 hidden attention layers - in the broad spirit of the [Cybenko theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)).\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1CJu1izJWYjKArukjYHzw1Hw9KGYhRKlk\" width=65%>\n","\n","It is to be emphasized, that __induction heads are geared towards in context learning__, so the strong instruction following performance of modern (especially instruction finetuned) LLMs is vindicated. In a sense, one can argue, that this is a case of adding __human bias__ to the models - albeit in a __positive sense__!\n","\n","For a more in-depth introduction see:\n","\n","[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)\n","\n","and"]},{"cell_type":"code","execution_count":null,"id":"c9b32448","metadata":{"id":"c9b32448","outputId":"e169e186-7274-41b8-983f-7010cc6b5114","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1701259643684,"user_tz":-60,"elapsed":367,"user":{"displayName":"Florian Ellsaesser","userId":"02356441736154567181"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pC4zRb_5noQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},"metadata":{}}],"source":["%%HTML\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/pC4zRb_5noQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},{"cell_type":"markdown","id":"56e89142","metadata":{"id":"56e89142"},"source":["\n","(For other curios phenomena, like the \"SolidGoldMagikarp\" phenomena of __\"glitch tokens\"__, and for a better in-depth understandign of the trained vectorspace see [here](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation#A_possible__partial_explanation).)"]},{"cell_type":"markdown","id":"7935634d","metadata":{"id":"7935634d"},"source":["## Too big to fail? - Model sizes and scaling laws\n","\n","With the advent of transformers, \"scaling\" in terms of parameters became possible. With the GPT line of work, and the arising of the \"zero shot paradigm\" it became a widespread experience, that larger models offer better performance.\n","\n","Thus, __scaling to larger and larger sizes became the imperative__, and a kind of \"size race\" ensued.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1yB8MgigkQgyGY-BdwooGp84auRwRW9ID\" width=65%>\n","\n","[source](https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=14nvvS_xdSpzFTFQKJq4hbwZYxwqS1r0b\" width=65%>\n","\n","[source](https://lifearchitect.ai/models/)\n","\n","The assumption, that the size of the models is crucial for their performance was first formalized in the paper [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1K4cRzFEy5F4vjzp1RSo3iAdVn7L1cIf8\" width=75%>\n","\n","The paper - and the substantial amount of followup work - established a notion, that training larger and larger models on increasingly huge datasets predictably increases perfromance.\n","\n","Later on, though, in th work [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf) it was found, that maybe training dataset size has stronger influence on the final result than the sheer model size. ([Later work](https://github.com/EleutherAI/pythia) also seems to corroborate this finding.)\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1fzH_tmjFdGGYIOZJkdZ4Y5vdDWYEgh0W\" width=45%>\n","\n","Source: [Chinchilla's wild implications](https://www.alignmentforum.org/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications)\n"]},{"cell_type":"markdown","id":"254f21ec","metadata":{"id":"254f21ec"},"source":["### Do we REALLY need that big of a scale?\n","\n","In spite of the popularity of the notion, that we need huge scales in number of parameters for complex behavior to \"emerge\" in Large Language Models, some evidence is being collected to the contraty:\n","\n","The research presented recently in the paper [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/abs/2304.15004) points towards a different explanation, namely that there is no \"sudden appearance\" of more sophisticated abilities, just as we scale models, they get gradually better and better (we just measure them with performance metrics that skew the result, eg. there is non-zero performance of zero shot learning in small models also, just it is not expressible in terms of accuracy).\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1AkwWXPzyInjV4InRh92spJHx0BKepZs4\" width=65%>\n","\n","If this turns out to be true, we might be tempted to conclude, that size is not the only factor that gives good performance for LLMs, but especialy the instruction finetuning seems to be a reasonable bias towards what we consider useful. This would mean, that we could potentially get away with way smaller model sizes than currently employed, which is a benefit, since the sheer scale of state-of-the-art models is in itself a barrier, thus hampering open innovation."]},{"cell_type":"markdown","id":"ef03cae9","metadata":{"id":"ef03cae9"},"source":["# How to use LLMs in practice?"]},{"cell_type":"markdown","id":"53734b60","metadata":{"id":"53734b60"},"source":["## Prompts"]},{"cell_type":"markdown","id":"904da828","metadata":{"id":"904da828"},"source":["### How to write good prompts?\n","\n","Motto:\n","\n","__Context is everything, keep the context \"in mind\"!__\n","\n","#### Anatomy of a prompt\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=15NXmYlHop5H47nsTFE1woojWJOJmaMM_\" width=90%>\n","\n","#### \"Formalization\"\n","\n","Also, \"formalization\" helps. The usage of markings and structuring aids in:\n","- clarifies roles\n","- creates \"slots\" (which will be of crucial importance!\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1x7U3ylG9vrjNSELraoKhIV7NPnjB0IJw\" width=90%>\n","\n","#### Example\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1uMt0r5RRrX5d82qHkbN1Sng7XvDxD29X\" width=90%>\n","\n","__More about promting [here](https://www.youtube.com/watch?v=xnXDsYquT5A&list=PLey6rpDz-3B9fymKhp_TPOomx-1_iJxCY).__"]},{"cell_type":"markdown","id":"a690975a","metadata":{"id":"a690975a"},"source":["### Simple strategies for building prompts\n","\n","For the manual construction of prompts some simple strategies can help a lot:\n","\n","- __Trial and error__: iterate, refine continuously\n","- __Refinement__: Start simple, gradually add complexity\n","- __Decomposition__: Try to decompose tasks to smaller elements\n","- __\"Nesting\"__: From smaller elements, compose more and more high level solutions\n","\n","We will see some more elaborate \"reasoning\" below, why these \"decomposition\" and \"step-by-step\" approaches can work well."]},{"cell_type":"markdown","id":"30d1f976","metadata":{"id":"30d1f976"},"source":["#### \"Reverse prompting\"\n","\n","Since prompting is basically goal oriented creative work in the domain of words, practitioners soon realized, that one can use an LLM (given some solved examples) to generate some prompts for LLMs (the same or different) to solve the given task. This approach can be called \"reverse prompt engineering\".\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1Nv9HaWQWRwk4JDBn1QQ1extuL4UiMMhf\" width=45%>\n","\n","[source](https://the-prompt-engineer.beehiiv.com/p/8-reverse-prompt-engineering)\n","\n","(It is important to note, that a \"jailbreaking\" technique exists with the same name, derived from \"prompt injection\". For more details on that, see [here](https://www.latent.space/p/reverse-prompt-eng).)\n","\n","This approach already points towards the fact, that prompting can be nderstood as a \"search\" problem, hence the apparatus of computer science (namely search and optimization algorithms) can be unleashed upon it."]},{"cell_type":"markdown","id":"6f97a100","metadata":{"id":"6f97a100"},"source":["### The Search for prompts"]},{"cell_type":"markdown","id":"-nZJy8NOG3uF","metadata":{"id":"-nZJy8NOG3uF"},"source":["#### Motivation and context: Controlling Neural Text Generation"]},{"cell_type":"markdown","id":"9tw5WBBd31MP","metadata":{"id":"9tw5WBBd31MP"},"source":["**Example**: the Frankfurt School wants to write an email to potential MBA students convincing them to join the MBA programme\n","\n","There are three requirments\n","\n","1.   The language model has to use content that is true about the Frankfurt School\n","2.   The text should be as effective as possible in reaching its goal - in our case applications to the School's MBA programme\n","3.   In many cases there are only few examples of texts and target values (click through rates) available\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"BPz-JOTXG6uz","metadata":{"id":"BPz-JOTXG6uz"},"source":["**The setting**\n","- If LLMs are creating consistent answers of high quality a key question is how to get the answer required for the particular business or research context (closely related to grounding)\n","- Task may include getting text that meets particular (1) content, (2) style and (3) quantitative metrics, such as optimizing for a click through rate\n","- Sometimes it may be necessary to build a model that completes this task: generating a prompt that fits with our objective\n","- The less labled examples we need to achieve the target the more valuable is the business application\n","\n","\n"]},{"cell_type":"markdown","id":"pk78OaRB3RN5","metadata":{"id":"pk78OaRB3RN5"},"source":["**Typical process for controlling neural text generation**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1253xgMcQ8R8dtZRWX-vD3LT-9sOjUmm4\" width=800 heigth=800>\n","\n","\n","[source](https://arxiv.org/pdf/2201.05337.pdf)"]},{"cell_type":"markdown","id":"bN4eyl5H4sQo","metadata":{"id":"bN4eyl5H4sQo"},"source":["**Taxonomy of control conditions**\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1Qst_n8NZu42YYr_ab9OyHTKiLhaefdII\" width=600 heigth=600>\n","\n","\n","[source](https://arxiv.org/pdf/2201.05337.pdf)\n","\n"]},{"cell_type":"markdown","id":"sxoJHQivcDO3","metadata":{"id":"sxoJHQivcDO3"},"source":["#### Hard prompts with access to the output values of the model"]},{"cell_type":"markdown","id":"SxPRQ5-5aMk_","metadata":{"id":"SxPRQ5-5aMk_"},"source":["Generally two key factors to consider:\n","1.   How the space of possible prompts is searched / pre-selected\n","2.   How to evaluate the performance of proposed prompts"]},{"cell_type":"markdown","id":"9Cj5K5YwDjhy","metadata":{"id":"9Cj5K5YwDjhy"},"source":["##### Permutation based - auto- prompt\n"]},{"cell_type":"markdown","id":"oJeqeH0FDqRe","metadata":{"id":"oJeqeH0FDqRe"},"source":["[**AUTOPROMPT: Eliciting Knowledge from Language Models\n","with Automatically Generated Prompts**](https://arxiv.org/pdf/2010.15980.pdf)\n","\n","- Add to the input sequence a sequence of additional tokens (called trigger tokens) that can be learned is added to the prompt\n","\n","\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1oyLil3lppN84reSi1paX7NgpgszinSzu\" width=700 heigth=700>"]},{"cell_type":"markdown","id":"8-ZL1DPEF116","metadata":{"id":"8-ZL1DPEF116"},"source":["**How to optimize**\n","\n","- Requires access to vector outputs of model at each level (embeddings)\n","\n","- Feed prompt into the language model produces - measure probability distribution $p\\left([\\mathrm{MASK}] \\mid \\boldsymbol{x}_{\\text {prompt }}\\right)$ over mask tokens (in this case negative vs positive sentiment)\n","\n","- At each step compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the $j$ th trigger token $x_{\\text {trig }}^{(j)}$ with another token $w \\in \\mathcal{V}$\n","\n","-  Identify a candidate set $\\mathcal{V}_{\\text {cand }}$ of the top- $k$ tokens estimated to cause the greatest increase:\n","$$\n","\\mathcal{V}_{\\text {cand }}=\\text { top } k\\left[\\boldsymbol{w}_{\\text {in }}^T \\nabla \\log p\\left(y \\mid \\boldsymbol{x}_{\\text {prompt }}\\right)\\right]\n","$$\n","\n","- Chose best candiates.."]},{"cell_type":"markdown","id":"Pdxro6T2a-bD","metadata":{"id":"Pdxro6T2a-bD"},"source":["Further development using similar techniques:\n","[Making Pre-trained Language Models Better Few-shot Learners\n","](https://arxiv.org/pdf/2012.15723.pdf\n",")"]},{"cell_type":"markdown","id":"OJBMhQ2mdG8L","metadata":{"id":"OJBMhQ2mdG8L"},"source":["##### Genetic algorithm based"]},{"cell_type":"markdown","id":"9zrD-zvignbn","metadata":{"id":"9zrD-zvignbn"},"source":["**[GPS: Genetic Prompt Search for Efficient Few-shot Learning](https://arxiv.org/pdf/2210.17041.pdf)**\n"]},{"cell_type":"markdown","id":"jG4qz1T3gZgI","metadata":{"id":"jG4qz1T3gZgI"},"source":["**Basic idea**\n","- Use some heuristics to generate new prompts from an inital one\n","- Chose top prompts through a fitness function (genetic algorithm inspired)\n","\n"]},{"cell_type":"markdown","id":"LNCds4RtgZcb","metadata":{"id":"LNCds4RtgZcb"},"source":["\n","<img src=\"http://drive.google.com/uc?export=view&id=18pR6S9WG-4ZnNJUogsu-n0ZJ3x-BTetH\" width=700 heigth=700>\n"]},{"cell_type":"markdown","id":"CKmEyZTagZYM","metadata":{"id":"CKmEyZTagZYM"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1KHOYEzRP8Cxcuj3AwIxXBl0joUtLc3q1\" width=350 heigth=350>\n","\n"]},{"cell_type":"markdown","id":"DlDUYEd_gZAv","metadata":{"id":"DlDUYEd_gZAv"},"source":["**How are prompts generated?**\n","- Back Translation\n","- Cloze\n","- Sentence continuation"]},{"cell_type":"markdown","id":"VZFGGCVrkn8p","metadata":{"id":"VZFGGCVrkn8p"},"source":["**Evaluation**:\n","Score on accuracy for the required task for the train dataset (no surrogate model)"]},{"cell_type":"markdown","source":["Another approach is to - [prompt Engeneering the prompt engeneer](https://arxiv.org/pdf/2311.05661.pdf)\n","\n","\n","The goal of prompt engineering. Prompt engineering is the process of finding the best textual prompt that can guide a large language model (LLM) to perform a given task, such as translation, classification, or reasoning. Prompt engineering can be challenging and time-consuming, and often requires manual trial-and-error efforts.\n","\n","The framework of automatic prompt engineering with LLMs. The authors propose a general framework that uses LLMs themselves to propose new and improved prompts. The framework consists of three components: prompt initialization, new prompt proposal, and search procedure. The new prompt proposal is guided by a meta-prompt, which is a text instruction that tells the LLM how to inspect the current prompt and a batch of examples, and how to generate a new prompt.\n","\n","The main contributions of this paper. The paper focuses on constructing a better meta-prompt that can lead to better prompt engineering quality. The paper introduces and analyzes various meta-prompt components, such as a step-by-step reasoning template, context specification, and optimizer-inspired concepts. The paper also presents a method called PE2, which combines the best-performing components and achieves strong performance on diverse language tasks.\n","\n","<a href=\"https://arxiv.org/pdf/2311.05661.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1zHzhYAI54Hn_NOab4WHfPll9oU0hjQi2\" width=60%></a>\n","\n"],"metadata":{"id":"oa0GMfP9lImD"},"id":"oa0GMfP9lImD"},{"cell_type":"markdown","id":"wKiPczENalOj","metadata":{"id":"wKiPczENalOj"},"source":["##### Reinforcement learning based methods\n","\n","1. How the space of possible prompts is searched/ pre-slected : **RL**\n","2. Evaluating performance of proposals: **Train separate classifier or Use Existing language model/ fine tune for classification**"]},{"cell_type":"markdown","id":"L1eEWht_bPIT","metadata":{"id":"L1eEWht_bPIT"},"source":["[**Efficient (Soft) Q-Learning\n","for Text Generation with Limited Good Data**](https://arxiv.org/pdf/2106.07704.pdf)\n","\n","- Example of this approach\n","- Uses RL for generating suitable prompts\n","- Trains classifier to evaluate the suitabliy of changed prompt"]},{"cell_type":"markdown","id":"rgBiCeudoEIj","metadata":{"id":"rgBiCeudoEIj"},"source":["**Motivation of the paper**\n","\n","- *MLE (Maximum likelihood estimates)* need large amounts of supervised data -> problem when literally no supervised data is available\n","- *RL learning* advantage: learning reward function, for discrete non differentiable events\n","- *RL Problems:* unstable/ notoriously difficult to train: (1) sparse reward: only once text is finished; (2) large action space - vocab of millions of words  "]},{"cell_type":"markdown","id":"nx4y0xw-73p3","metadata":{"id":"nx4y0xw-73p3"},"source":["<img src=\"http://drive.google.com/uc?export=view&id=1PVuwAoFTXN1peGTGrGUmmrcjWczYwXQS\" width=600 heigth=600>"]},{"cell_type":"markdown","id":"vuOPIqbWCA6P","metadata":{"id":"vuOPIqbWCA6P"},"source":["**Propose *soft* Q-Learning**\n","\n","-  Maximum-entropy (MaxEnt) extension to the standard (hard) Q-learning\n","- Agent is encouraged to optimize the reward while staying as stochastic as possible´\n","- Objective $J_{\\operatorname{MaxEnt}}(\\pi)=$ $\\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^T \\gamma^t r_t+\\alpha \\mathcal{H}\\left(\\pi\\left(\\cdot \\mid \\boldsymbol{s}_t\\right)\\right)\\right]$\n","- Augments the vanilla $J(\\pi)$ with the additional Shannon entropy term $\\mathcal{H}$ with coefficient $\\alpha $\n","- Connects $Q$-values to the familiar output logits of a text generation model, which enables straightforward implementation of the SQL formulation"]},{"cell_type":"markdown","id":"clZ-Q3xGxnK_","metadata":{"id":"clZ-Q3xGxnK_"},"source":["**Soft q-learning replaces the argmax operator with a Softmax**\n","- Connection of the $Q$-values with the logits, i.e., outputs right before the softmax layer.\n","- Following relationship between optimal policy $\\pi^*$ and action-value $Q^*$ holds (Haarnoja et al., 2017; Schulman et al., 2017):\n","$$\n","\\pi^*(a \\mid s)=\\frac{\\exp Q^*(\\boldsymbol{s}, a)}{\\sum_{a^{\\prime}} \\exp Q^*\\left(\\boldsymbol{s}, a^{\\prime}\\right)} .\n","$$\n","This form is highly reminiscent of the softmax layer of the generation model\n","\n","- **Key point:** the softmax leads to exploration, but not in an epsylon greedy style where each non argmax action is equally likely -> no arbitrary sampling- this is language after all!\n","- The temperature of the softmax can be decreased over time to to make the model converge\n"]},{"cell_type":"markdown","id":"IAiEdmQwyU8E","metadata":{"id":"IAiEdmQwyU8E"},"source":["**Connection to softmax 2**\n","\n","In other words, the model output $f_\\theta(a \\mid s)$, originally interpreted as the \"logit\" of token a given the preceding tokens $s$, is now re-interpreted as the $Q$-value of action $a$ in state $s$. When achieving optimality, $f_{\\theta^*}(a \\mid s)$, namely $Q^*(s, a)$, represents the best possible future reward achievable by generating token $a$ in state $s$. Similarly, the full generation $\\operatorname{model} p_\\theta(a \\mid \\boldsymbol{s})$ in Eq. (1) that applies softmax to $f_\\theta$ now precisely corresponds to the policy $\\pi_\\theta$ induced from $Q_\\theta(s, a)$. That is,\n","$$\n","\\begin{aligned}\n","\\pi_\\theta(a \\mid \\boldsymbol{s}) & =\\frac{\\exp Q_\\theta(\\boldsymbol{s}, a)}{\\sum_{a^{\\prime}} \\exp Q_\\theta\\left(\\boldsymbol{s}, a^{\\prime}\\right)} \\\\\n","& \\equiv \\frac{\\exp f_\\theta(a \\mid \\boldsymbol{s})}{\\sum_{a^{\\prime}} \\exp f_\\theta\\left(a^{\\prime} \\mid \\boldsymbol{s}\\right)}=p_\\theta(a \\mid \\boldsymbol{s}) .\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","id":"piRKk-11zcuD","metadata":{"id":"piRKk-11zcuD"},"source":["**Additionally effective training with path consistency**\n","\n","- Adapt unified path consistency [learning $(P C L)$](https://dl.acm.org/doi/pdf/10.5555/3294996.3295037) (excelled in game control)\n","- Good for directly learning RL algorithm on past data\n","- Simple value based reinfocement learning - does not require actor critique\n","- PCL-based training updates $Q$-values of all tokens at once through a connection between the value function and the induced policy\n","-  Optimal policy $\\pi^*$ and the optimal state value function $V^*$  in SQL must satisfy the following consistency property for all states and actions:\n","$$\n","V^*\\left(\\boldsymbol{s}_t\\right)-\\gamma V^*\\left(\\boldsymbol{s}_{t+1}\\right)=r_t-\\log \\pi^*\\left(a_t \\mid \\boldsymbol{s}_t\\right), \\forall \\boldsymbol{s}_t, a_t\n","$$\n","\n","- Accordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective $\\mathcal{L}_{\\mathrm{SQL}, \\mathrm{PCL}}(\\boldsymbol{\\theta})$ :\n","$$\n","\\mathbb{E}_{\\pi^{\\prime}}\\left[\\frac{1}{2}\\left(-V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_t\\right)+\\gamma V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_{t+1}\\right)+r_t-\\log \\pi_\\theta\\left(a_t \\mid \\boldsymbol{s}_t\\right)\\right)^2\\right],\n","$$\n","where $\\pi_\\theta$ is the induced policy; $V_{\\bar{\\theta}}$  depends on the target $Q_{\\bar{\\theta}}$ network (i.e., a slow copy of the $Q_\\theta$ to be learned), and recall that $\\pi^{\\prime}$ is an arbitrary behavior policy (e.g., data distribution). Please see"]},{"cell_type":"markdown","id":"aaqh5lFI7-jQ","metadata":{"id":"aaqh5lFI7-jQ"},"source":["\n","<img src=\"http://drive.google.com/uc?export=view&id=115CR08rGbOpmKxu1YIg-b9ugvT8b-RU5\" width=600 heigth=600>"]},{"cell_type":"markdown","id":"FBH3yxX41SZp","metadata":{"id":"FBH3yxX41SZp"},"source":["**Multi-step PCL for Sparse Reward**\n","$$\n","V^*\\left(\\boldsymbol{s}_t\\right)-\\gamma^{T-t} V^*\\left(s_{T+1}\\right)=\\sum_{l=t}^T \\gamma^{l-t}\\left(r_l-\\log \\pi^*\\left(a_l \\mid \\boldsymbol{s}_l\\right)\\right),\n","$$\n","where the value of past-terminal state is zero, $V^*\\left(s_{T+1}\\right)=0$; and the rewards are only available at the end, $\\sum_{l=t}^T \\gamma^{l-t} r_l=\\gamma^{T-t} r_T$. We can then come to the following multi-step objective function $\\mathcal{L}_{\\mathrm{SQL}}$ PCL-ms $(\\boldsymbol{\\theta})$,\n","$\\mathbb{E}_{\\pi^{\\prime}}\\left[\\frac{1}{2}\\left(-V_{\\bar{\\theta}}\\left(\\boldsymbol{s}_t\\right)+\\gamma^{T-t} r_T-\\sum_{l=t}^T \\gamma^{l-t} \\log \\pi_\\theta\\left(a_l \\mid \\boldsymbol{s}_l\\right)\\right)^2\\right]$.\n","We can see the objective side-steps the need to bootstrap intermediate value functions $V_{\\bar{\\theta}}\\left(s_{t^{\\prime}}\\right)$ for $t^{\\prime}>t$. Instead, it directly uses the non-zero end reward $r_T$ to derive the update for $\\boldsymbol{\\theta}$. Please see Figure 2 (right) for an illustration. In practice, we combine the single- and multi-step objectives (Eqs. 7 and 9) together for training."]},{"cell_type":"markdown","id":"mWNuxDzN5FIf","metadata":{"id":"mWNuxDzN5FIf"},"source":["**Reward**\n","we use a distilled GPT-2 model\n","as the pretrained LM to be controlled.\n","For rewards, we use the topic accuracy of the con\u0002tinuation sentences measured by a zero-shot clas\u0002sifier, plus the the log-likelihood of continuation\n","sentences as the language quality reward measured\n","by a distilled GPT-2"]},{"cell_type":"markdown","id":"jw2Tykxw6Vb-","metadata":{"id":"jw2Tykxw6Vb-"},"source":["\n","<img src=\"http://drive.google.com/uc?export=view&id=1ezZ4PCL2LGJl75JG_TAI_aQqmG9Tuv0e\" width=600 heigth=600>\n"]},{"cell_type":"markdown","id":"sazE2UPXz7qT","metadata":{"id":"sazE2UPXz7qT"},"source":["Other RL based approaches:\n","[RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning\n","](https://arxiv.org/abs/2205.12548)"]},{"cell_type":"markdown","id":"bCZTh29gcSFp","metadata":{"id":"bCZTh29gcSFp"},"source":["####  Soft prompts with machine learning/optmization\n","\n","- Soft as they are not discrete (numbers in the vectors can be changed gradually)\n","\n","see for example the following paper on [prefix tuning](https://aclanthology.org/2021.acl-long.353/)"]},{"cell_type":"markdown","id":"871886b6","metadata":{"id":"871886b6"},"source":["#### What if we don't want to deal with gradients? - Evolution to the rescue\n","\n","The discrete nature of prompts - since they are ideally based on discrete tokens - makes the application of gradient based procedures on them rather cumbersome. Luckily optimization literature has a rich set of non-gradient based optimizers that can come in handy for this task.\n","\n","One family of such optimizers is \"evolutionary computation\", that [this paper](https://arxiv.org/abs/2309.08532) proposes to use in automated prompt tuning.\n","\n","The basic idea of an evolutionary algorithm is to use a __population of solutions, that undergo evolutionary operators, that is:\n","- __Selection__\n","- __Crossover__\n","- __Mutation__\n","\n","to produce an ever increasing quality.\n","\n","(see details in the addon material about optimization)\n","\n","The general framework is applied to prompts as follows:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1ubC0uDwbB7RsjgXzrq2IKU8SFoam1rr6\" width=55%>\n","\n","__Some notable modifications of the general algorithm:__\n","\n","- \"__Initial population__: Based on our notation that most existing prompt-based methods neglect human knowledge providing efficient priori initialization, we apply several manual prompts as the initial population to leverage the wisdom of humans as prior knowledge. Besides, EAs typically start from randomly generated solutions, resulting in a diverse population and avoiding being trapped in a local optimum. Accordingly, we also introduce some prompts generated by LLMsinto the initial population.\"\n","- __Selection__ is done in a standard \"roulette wheel\" fashion\n","\n","\n","Maybe the most important contribution of the paper is to elaborate two approaches for the \"crossover\" and \"mutaiton\" operators, that are heavily utilizing LLMs themselves for an intelligent, guided approach for searching in the neighborhood of good solutions.\n","\n","One is based on \"classical\" evolution:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1xPag5h3pdO0EdIXHpjLLYFpAVSBP7gYR\" width=85%>\n","\n","The other is based on \"differential evolution\":\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1xFvAnaXUE0Cg505KPrtm0TrRcE0kBApO\" width=85%>\n","\n","Both approaches seem to offer superior performance to other prompt searching methods, and most importantly to human manual prompting baselines.\n"]},{"cell_type":"markdown","id":"44330c74","metadata":{"id":"44330c74"},"source":["### Still fails sometimes!\n","\n","#### Hallucination\n","\n","\n","**Language models are trained on a common corpus including texts from the internet, predicting the next word**\n","\n","**We have language models, not knowledge models**\n","\n","\n","A major form of bad performance - and a frivolously often cited shortcoming - of earlier LLMs was hallucination, especially in numeric and reasoning type tasks.\n","\n","Some vocal critics of the LLM paradigm went so far as to call the whole paradigm \"flawed\" and \"useless\" based on the poor performance of (mainly non instruction finetuned) LLMs on reasoning tasks.\n","\n","This resulted in considerable focus from the side of the scientific community.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1dkUAL_WV1TtZJCnZhfMfGCqtAr9vl44Z\" width=65%>\n","\n","In the work [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) the authors realized, that instead of the final task, the __LLM is made to produce a step-by-step reasoning__ for the task solution, it's __final task performance becomes significantly better__!\n","\n","This lead to \"chain-of-thought prompting\" to become the de-facto standard solution strategy.\n","\n","##### \"Let's think step by step!\"\n","\n","In the work [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf) it is shown, that this kind of strategy, namely only hinting towards chain of thought in the prompt for an LLM in a task can ellicit strong reasoning performance.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1uyJR4LNzRVsh1dYpAevHyI3YP1t8VgvG\" width=75%>\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1GQJDP4cGJpV8JCMh5Ggk1TeYxNS8TnvG\" width=65%>\n","\n","\"Let's think step-by-step!\" became a standard part in nearly every well \"engineered\" prompt.\n","\n","#### \"Mind in a box\"\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1GYEV_6oWcoXolIe0qDVTUhePwn_2icmc\" width=45%>\n","\n","As illustrated by the image above, LLMs by default, when the pretraining and finetuning is finished are \"frozen\" in time (since continuous training _as of now_ is computaitonally infeasible), so the act like __\"minds in a box\"__ that have no access to the outside world.\n","\n","\n","In many (actually dominantly many) real life tasks the models ideally should have up to date information, and access to it's sources. The paradigm of [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) - and the many followup works based upon it - __give access to an external knowledge base of facts - or later the whole internet - via a search engine__ that helps provide rich context for the LLMs to solve the tasks.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1p7Ajkz_QnzhhI6zzUAgdtKKP04kPyVln\" width=45%>\n","\n","The whole paradigm of information retrieval - mainly via the help of high quality, contextual, neural embeddings - comes to aid the LLM performance, and defines the new state-of-the-art.\n","\n","On more recent advancements in this area see the great survey: [Augmented Language Models: a Survey](https://arxiv.org/abs/2302.07842)\n","\n","The kind of \"procedural\" reasoning ability, combined with the inspired research towards the external resources lead to a new paradigm.\n"]},{"cell_type":"markdown","id":"3d8033eb","metadata":{"id":"3d8033eb"},"source":["## \"Agents\" and \"Tools\" - access the \"outside world\"\n","\n","\n","### Basic idea: ReACT\n","\n","Based on the emergent reasoning abilities and the introduction of external \"sources\", the authors of the paper [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629) went a step further, and proposed a solution in which the LLMs themselves are presented with a set of \"tools\", and asked to do a \"step-by-step palnning\" of how would they use the tools. The addon here is: __the planned steps of tool usage gets executed by a framework__, so if the LLM decides that it should search for a thing on the internet via a \"search engine tool\", the environment executes this \"query\" and gives back the result (in textual, \"serialized\" form) to the LLM.\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=10HLujjZ-lZsyrlbhjNp_HofY_lAm1iNw\" width=85%>\n","\n","This way, the __LLM acts as an autonomuos agent, formulating \"plans\" and using \"tools\"__.\n","\n","\n","### Result: Tool ecosystems\n","\n","As a result pf this paradigm, a __huge ecosystem__ of tools emerged, that aim to connect external services to LLMs, thus add extraordinary capabilities, and open up the floodgate for practical applications.\n","\n","The two dominant - in a sense mutually reinforcing - ecosystems are:\n","\n","<img src=\"http://drive.google.com/uc?export=view&id=1uRcgPG1k_z00rXkWV4TG_pm_HkGbuzqc\" width=80%>\n","\n","Read more about:\n","- [ChatGPT plugins](https://openai.com/blog/chatgpt-plugins)\n","- [Langchain](https://langchain.readthedocs.io/)"]},{"cell_type":"code","execution_count":null,"id":"b2c3ef2e","metadata":{"id":"b2c3ef2e","outputId":"af603da6-97e0-440f-def6-632e26418b1d","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1701262277566,"user_tz":-60,"elapsed":5,"user":{"displayName":"Florian Ellsaesser","userId":"02356441736154567181"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nE2skSRWTTs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},"metadata":{}}],"source":["%%HTML\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/nE2skSRWTTs\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},{"cell_type":"markdown","source":["#### More advanced approach of planning by iteratively decomposing tasks\n","\n","**ADAPT**: A novel approach for using large language models (LLMs) for interactive decision-making tasks that require planning and adapting to the environment.\n","ADAPT uses a recursive algorithm that dynamically decomposes complex tasks into simpler sub-tasks as-needed, based on the capabilities of the executor LLM.\n","\n","**Executor and Planner:** Two LLM-based modules that perform low-level execution and high-level planning, respectively. The executor interacts iteratively with the environment using actions generated by the LLM, while the planner breaks down complex tasks into a few steps and logical operators using the LLM’s world knowledge.\n","\n","**Controller:** An LLM program that integrates the executor and planner modules within ADAPT. The controller assigns a task to the executor first, and if the executor fails, it calls the planner to generate a plan and recursively calls ADAPT for each sub-task until a termination criterion is met.\n","\n","<a href=\"https://arxiv.org/abs/2311.05772\"><img src=\"https://drive.google.com/uc?export=view&id=1JGKbvE4om0ZA4yu2XYx0zucgrdgem9sV\" width=60%></a>\n"],"metadata":{"id":"fO6Tqx7ulzWv"},"id":"fO6Tqx7ulzWv"},{"cell_type":"markdown","source":["#### Autonomy through tools\n","\n","\n","<a href=\"https://www.latent.space/p/agents\"><img src=\"https://drive.google.com/uc?export=view&id=1bA1ZSyU9VZFhg6U4r31FJXBdqiWLRm0o\" width=80%></a>\n","\n","\n","see\n","- [here](https://www.latent.space/p/agents)\n","- [here](https://medium.com/@liao.annie.2000/investment-thesis-debundling-the-market-landscape-the-rise-of-autonomous-ai-agents-ae618e5ff07e)\n","- [here](https://lilianweng.github.io/posts/2023-06-23-agent/)\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"EPx4iS7PQ4iY"},"id":"EPx4iS7PQ4iY"},{"cell_type":"markdown","id":"5b8e4d7c","metadata":{"id":"5b8e4d7c"},"source":["#### Learning new tools\n","\n","In a \"logical\" continuation of this paradigm, people started to experiment with models, that were capable of __learning the usage of new tools__ like in [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761). The gust here is, that an appropriately trained, but also just an __appropriately prompted__ model can dynamically adapt (eg. via API description texts) to a new tool environment.\n","\n","\n","This lead to experiments in enhanced autonomy, like:\n","- [BabyAGI](https://www.youtube.com/watch?v=QBcDLSE2ERA)\n","- [AutoGPT](https://www.youtube.com/watch?v=LqjVMy2qhRY) and\n","- [HuggingGPT](https://www.youtube.com/watch?v=PfY9lVtM_H0)\n","\n","__These videos are definitely worth a look__, since they represents the absolute state-of-the-art.\n","\n","They would merit a more thorough elaboration..."]},{"cell_type":"markdown","source":["The latest advacement in this paradirm have been **multi agent systems** such as Microsoft Autogen\n","\n","\n","\n","<a href=\"https://www.microsoft.com/en-us/research/project/autogen/\"><img src=\"https://drive.google.com/uc?export=view&id=1Z19ft8ifFjhj1YgJ_Yt5Kkx3CN5NT8iO\" width=80%></a>\n"],"metadata":{"id":"Um913udrJV2Y"},"id":"Um913udrJV2Y"},{"cell_type":"code","source":["%%HTML\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Zlgkzjndpak\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"EtyOawnLS9uE","executionInfo":{"status":"ok","timestamp":1701265329055,"user_tz":-60,"elapsed":8,"user":{"displayName":"Florian Ellsaesser","userId":"02356441736154567181"}},"outputId":"b5ff9824-56ed-431f-baaa-a76d2c2cd2f4"},"id":"EtyOawnLS9uE","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Zlgkzjndpak\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},"metadata":{}}]},{"cell_type":"code","source":["%%HTML\n","<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5Zj_zstLLP4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"LCoMguVrTu-I","executionInfo":{"status":"ok","timestamp":1701265417821,"user_tz":-60,"elapsed":306,"user":{"displayName":"Florian Ellsaesser","userId":"02356441736154567181"}},"outputId":"55fc6adf-71b3-4f9c-a835-07a4365bd748"},"id":"LCoMguVrTu-I","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5Zj_zstLLP4\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"]},"metadata":{}}]},{"cell_type":"markdown","id":"1ce96d72","metadata":{"id":"1ce96d72"},"source":["### Additonal notes\n","\n","This vision - and in fact reality - of trained neural agents working in cooperation to achieve a common complex goal very much resembles [Minsky's society of mind](https://en.wikipedia.org/wiki/Society_of_Mind) hypothesis, and gives some strong, empiric evidence in favour of it. It is basically the current incarnation of the dream of \"atonomuos software agents\" from decades ago.\n","\n","Also, it is important to stress, that with the advent of the T5 style text in - text out paradigm, and the prevalence of the zero shot approach, (nearly) __all interactions with the models are via text__.\n","\n","This __\"via language\" paradigm__ proved to be so general, that some researchers (eg. in [TabLLM: Few-shot Classification of Tabular Data with Large Language Models](https://arxiv.org/abs/2210.10723)) endeavored to cast \"traditional\", non-nlp problems into language in an attempt to solve them with LLMs.\n","\n","And since __software code is wirtten in human parseable languages___ (so as to enable us writing them), __the whole area of software development and software services became accessible for LLMs__. Natural language became the \"glue\", the \"communicaiton medium\" between humans, software, and in a sense amongst software. And as the famous saying goes [\"Software eats the world\"](https://a16z.com/2011/08/20/why-software-is-eating-the-world/)."]},{"cell_type":"markdown","id":"9530765e","metadata":{"id":"9530765e"},"source":["# Outlook - what to look for?\n","\n","- Scaling wars might stop (even OpenAI's [Sam Altman hints](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/) at this)\n","- Full OpenSource models will become (are) available (eg. [LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant))\n","- Multi-modality gets dominant (see for example [here](https://github.com/X-PLUG/mPLUG-Owl))\n","- Auto-coding gets prevalent (see for example [this](https://levelup.gitconnected.com/the-end-of-coding-experts-predict-a-future-of-automated-development-c0aae9c458a2) summary)\n","- __Business and personal integration will be the main challenge__\n","- Large language models can [ground reinfrocment learning models](https://arxiv.org/abs/2302.02662) and thus accelerate learning\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
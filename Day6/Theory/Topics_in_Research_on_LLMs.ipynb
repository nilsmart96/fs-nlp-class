{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyaxQYTNYsY_"
   },
   "source": [
    "# Motivation\n",
    "\n",
    "The success of LLMs has led to a spurt of research on:\n",
    "- Effectively re-training LLMS as part of a transfer learning task (see previous lecture)\n",
    "- Inner worknigs of LLMs / what do LLMs learn\n",
    "- Working on the limitations of LLMs, such as quadratic complexity\n",
    "- Using LLMs for additional tasks\n",
    "  - Multi-modality\n",
    "  - LLMs as optmizers\n",
    "- Looking at the capabilities and limitations of LLMs and to which extent they can lead to AGI\n",
    "- Are LLMs sufficient for AGI?\n",
    "\n",
    "We are going to cover each in turn what follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHF4Ib1IR3pQ"
   },
   "source": [
    "# 1. Inner workings of LLMs / what do LLMs learn\n",
    "\n",
    "\n",
    "\n",
    "## [Finding interpretable features by using sparse autoencoders](https://transformer-circuits.pub/2023/monosemantic-features/index.html)\n",
    "\n",
    "-\tTry to understand ever-larger models, the volume of the **latent space** representing the **model's internal state** that we need to interpret **grows exponentially**.\n",
    "-\tWe do not currently see a way to understand, search or enumerate such a space unless it can be decomposed into independent components, each of which we can understand on its own\n",
    "-\t**Superposition Hypothesis** states that **single neuron** (or rather ensembles of neurons) are used to **encode multiple features** rather than individual ones\n",
    "-\tFeature here means something about the world e.g. what language is being spoken, are we talking about a place and so on rather than input feature\n",
    "\n",
    "<a href=\"https://transformer-circuits.pub/2023/monosemantic-features/index.html\"><img src=\"https://drive.google.com/uc?export=view&id=1J9oJ0rYoj0VecMdrLRq8JxzCY6h3Fyav\" width=60%></a>\n",
    "\n",
    "-\tAs a result **looking at individual neurons, does not lead to understanding**\n",
    "-\tHowever, **directions in activations are  interpretable**, it's natural to think there's some **\"basic set\" of meaningful directions** which more **complex directions can be created from**.\n",
    "-\tWe call these **directions features**, and they're what we'd like to decompose models into.\n",
    "-\tThis is called **dictionary learning**\n",
    "- \"Dictionary\" (a set of elements) that can be used to represent data efficiently.\n",
    "- The basic idea is to find a sparse representation of input data using an overcomplete basis set.\n",
    "- In other words, **each data point is described as a linear combination of a few dictionary elements**.\n",
    "-\tIt is an **np-hard problem** as: we're asking to determine a **high-dimensional vector from a low-dimensional projection**. Put another way, we're trying to invert a very rectangular matrix\n",
    "-\tThe only thing which makes it possible is that we are looking for a high-dimensional vector that is sparse! This is the famous and well-studied problem of compressed sensing, which is NP-hard in its exact form. It is possible to store high-dimensional sparse structure in lower-dimensional spaces, but recovering it is hard.\n",
    "-\tTrains simple one layer transformer language model\n",
    "-\tUse spars categorical autoencoder on activation with higher-dimensionality than input in the middle to disentangle superposition\n",
    "\n",
    "<a href=\"https://transformer-circuits.pub/2023/monosemantic-features/index.html\"><img src=\"https://drive.google.com/uc?export=view&id=1HMYbCo70Un34xPStdudTYRehV6f0Bu0a\" width=60%></a>\n",
    "\n",
    "\n",
    "<a href=\"https://transformer-circuits.pub/2023/monosemantic-features/index.html\"><img src=\"https://drive.google.com/uc?export=view&id=13nt_-zVGir16KAbGmFMlq9zAkz-tasxZ\" width=60%></a>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx38NVbW6FwW"
   },
   "source": [
    "## Induction heads and in-context learning\n",
    "\n",
    "- [Argument](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html\n",
    " ) is that main in context learning feature of transformers is that as they are trained they develop special attention heads called **induction heads**\n",
    "\n",
    "Formally, we define an induction head as one which exhibits the following two properties on a\n",
    "repeated random sequence of tokens:\n",
    "- **Prefix matching:**-  The head attends back to previous tokens that were followed by the\n",
    "current and/or recent tokens. That is, it attends to the token which induction would suggest\n",
    "comes next.\n",
    "- **Copying:** The headʼs output increases the logit corresponding to the attended-to token.\n",
    "\n",
    "In other words, induction heads are any heads that empirically increase the likelihood of [B] given\n",
    "[A][B]...[A] when shown a repeated sequence of completely random tokens. An illustration of\n",
    "induction headsʼ behavior is shown here:\n",
    "\n",
    "\n",
    "<a href=\"https://www.anthropic.com/index/in-context-learning-and-induction-heads\"><img src=\"https://drive.google.com/uc?export=view&id=1dUHogj8WCBjsNU-8WHUyWvW49VvqUy0l\" width=60%></a>\n",
    "\n",
    "\n",
    "One of things weʼll be trying to establish is that when induction heads occur in sufficiently large\n",
    "models and operate on sufficiently abstract representations, the very same heads that do this\n",
    "sequence copying also take on a more expanded role of analogical sequence copying or in-context\n",
    "nearest neighbors. By this we mean that they promote sequence completions like\n",
    "[A*][B*] … [A] → [B] where A* is not exactly the same token as A but similar in some\n",
    "embedding space, and also B is not exactly the same token as B*\n",
    "\n",
    "\n",
    "Additoinal descirptions of how induction heads work:\n",
    "- [here](https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated)\n",
    "- [here](https://www.perfectlynormal.co.uk/blog-induction-heads-illustrated)\n",
    "- [here](https://www.anthropic.com/index/in-context-learning-and-induction-heads)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlJCuSpEA_vb"
   },
   "source": [
    "**More complex example of attention matching**\n",
    "\n",
    "- Show an attention head (found at layer 26 of the 40-layer model) which\n",
    "does more complex pattern matching. One might even think of it as learning a simple function in\n",
    "context!\n",
    "\n",
    "- To explore this behavior, generated some synthetic text which follows a simple pattern.\n",
    "- Eachline follows one of four templates, followed by a label for which template it is drawn from.\n",
    "- The template is randomly selected, as are the words which fill in the template:\n",
    "\n",
    "(month) (animal): 0\n",
    "\n",
    "(month) (fruit): 1\n",
    "\n",
    "(color) (animal): 2\n",
    "\n",
    "(color) (fruit): 3\n",
    "\n",
    "Below, we show how the attention head behaves on this synthetic example. To make the diagram\n",
    "easier to read, we've masked the attention pattern to only show the \":\" tokens are the destination,\n",
    "and the logit attribution to only show where the output is the integer tokens.\n",
    "\n",
    "\n",
    "<a href=\"https://www.anthropic.com/index/in-context-learning-and-induction-heads\"><img src=\"https://drive.google.com/uc?export=view&id=1XKrZHPtfmA4kADNrZU7DHHKpGxLFTEqn\" width=60%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caOwOiKz_qh4"
   },
   "source": [
    "# Multi modality\n",
    "\n",
    "\n",
    "Multimodal language model: understand and generate content across multiple modes of data, such as text, images, audio, and sometimes even video\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2309.05519.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1nK8uh3rz0B5tdp9eGPa4FxvSduV_V4nd\" width=60%></a>\n",
    "\n",
    "\n",
    "**The approach approach:**\n",
    "  - Centerpiece is a (pretrained) large language model of choice\n",
    "  - Encoders and decoders are pretrained models (such as diffusion models) for images, audio, video and any other modality\n",
    "  - To get a mapping from the the latent vector of the language model a projection layer is introduced (in a sense a vector mappig from the latent space of one model to the latent space of another model has to be learned)\n",
    "  - To get a mapping from the output of the languag model to the multi-modal decoders another mapping layer is introduced\n",
    "  - This way the total number of paparamters that need to be updated is limited\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqfaToqEUH3-"
   },
   "source": [
    "Depending on the modality different parts of the model are used\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2309.05519.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1ohvYG0m6HROeZGoQmTUG6h7OMTm-27jt\" width=60%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4CaKMi9QNex"
   },
   "source": [
    "**Training the model - How doe the training and loss work**\n",
    "\n",
    "There are usually 3 types of adjustments, that are made\n",
    "\n",
    "(1) For the encoder the information can be passed through the LLM to generate text which describes the input. This text can then be compared to original captionings\n",
    "\n",
    "(2) For the decoder image caption for the generated image, video, audio is passed into a text encoder, the resulting text can be compared to the image output projection\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2309.05519.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=17L2RhClSuclmFE_Njhs4XDhGJIzOSP62\" width=60%></a>\n",
    "\n",
    "(3) Involves additional\n",
    "training of overall MM-LLMs using ‘(INPUT, OUTPUT)’ pairs, where ‘INPUT’ represents the user’s\n",
    "instruction, and ‘OUTPUT’ signifies the desired model output that conforms to the given instruction.\n",
    "Technically, leverage LoRA to enable a small subset of parameters within NExT-GPT to be\n",
    "updated concurrently with two layers of projection during the IT phase (here parameters throughout the whole LLM are updated).\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2309.05519.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1zTjxcT_eGcfHshNEiZ_jvCkgcj5Dt4O9\" width=60%></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjrrdNzrEwEF"
   },
   "source": [
    "Along these lines the following [paper](https://arxiv.org/pdf/2311.02782.pdf) looks at multi modal anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dEabUvWzbgG"
   },
   "source": [
    "# LLMS as optmizers\n",
    "\n",
    "Idea:\n",
    "- Use the language model to directly optimize a task by prompting it accordingly (to optimze a task)\n",
    "- Iteravely improve the prompt\n",
    "\n",
    "[**Large language models as optimizers:**](https://arxiv.org/abs/2309.03409) Novel approach to use large language models (LLMs) as optimizers for various tasks, where the optimization problem is described in natural language and the LLM generates new solutions based on the problem description and the previously found solutions1.\n",
    "\n",
    "**OPRO framework:** Optimization by PROmpting (OPRO) framework, which consists of a meta-prompt that contains the optimization problem description and the optimization trajectory, and a solution generation step that leverages the LLM sampling temperature to balance between exploration and exploitation.\n",
    "\n",
    "**Case studies and applications:** demonstrate the potential of LLMs for optimization on two classic mathematical optimization problems: **linear regression and the traveling salesman problem**. Also apply OPRO to prompt optimization, where the goal is to find a prompt that maximizes the task accuracy for natural language processing tasks. Show that OPRO can consistently improve the performance of the generated prompts on several benchmarks, such as GSM8K and Big-Bench Hard, and outperform human-designed prompts by a large margin.\n",
    "\n",
    "-  The **GSM8K dataset**, short for Generative Spoken Model 8K, is a dataset specifically designed for training large language models to understand and generate human-like speech\n",
    "- The **BIG-bench-hard dataset** is an advanced and challenging benchmark designed for evaluating the capabilities of large language models, particularly in tasks that are difficult for current AI models. BIG-bench stands for \"Beyond the Imitation Game benchmark\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2309.03409\"><img src=\"https://drive.google.com/uc?export=view&id=1UYdD3FPchie2-vZtcznZGEnhw_mtWUdL\" width=60%></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Example of prompt\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/2309.03409\"><img src=\"https://drive.google.com/uc?export=view&id=1JhSsucVL7w5LVD4OGjhaai6Nqi_Rm3jR\" width=60%></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDXQYnOAYeLc"
   },
   "source": [
    "# Limitations of LLMs and how to adress them\n",
    "\n",
    "## LLMs and quadratic complexity of the attention mechanism\n",
    "**Attention scales with quadratic complexity**\n",
    "\n",
    "A number of approaches have been developed to deal with this issue\n",
    "- Change the attention mechanism e.g. do not attend all tokens (sparse attention) -covered previously\n",
    "- Storing some of the weights of attention in memory\n",
    "- Chunking and search (covered in information retrieval)\n",
    "- Using an LLM to summarize sub-components of a text into smaller pieces (in a search tree that can be used to retrieve appropriate text) - covered here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKx8KsuLojG1"
   },
   "source": [
    "[MEMWALKER](https://arxiv.org/pdf/2310.05029.pdf)(Memory Walker): An interactive reader - A method that uses a large language model (LLM) to read long texts and answer questions by building a memory tree and navigating it iteratively.\n",
    "\n",
    "Memory tree construction - A process that splits the long text into segments and summarizes them into nodes that form a tree structure. The LLM generates summaries using iterative prompting.\n",
    "\n",
    "Navigation - Upon receiving a query the model starts from the root node and traverses the tree to find the relevant segment for the query3. The LLM decides which node to inspect or revert to by generating reasoning and action.\n",
    "\n",
    "Note that this approach represents a traditional computer science based tree search approach but implemented with an LLM\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/2310.05029.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1eA6B2ba7iJzNX6hAa7ImVvZ4SNo5E7Mr\" width=60%></a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhA8YU1aRvVU"
   },
   "source": [
    "# Are LLMS sufficient for AGI?\n",
    "\n",
    "Beyond the successes of LLMs, there have been some critical voices on how close they bring us to AGI:\n",
    "\n",
    "A recent [paper](https://arxiv.org/pdf/2311.00871.pdf) by Google Deep Mind suggests that transformes\n",
    "- Are good for **in context learning** - perform new tasks when **prompted** with **unseen input-output examples** without any explicit model training as long as they are **close to the training data distribution**\n",
    "- But are not good if the **input-output examples** are **not close to the training data distribution**\n",
    "\n",
    "\n",
    "In the talk [\"Can LLMs Really Reason & Plan?\"](https://youtu.be/uTXXYi75QCU?si=6XLKu1kOjni_ALo5)  Subbarao Kambhampati argues that:\n",
    "- LLMS are N Gram on steroids 3000 grams\n",
    "- They are good at approximate look up of relevant \"knowledge\" they have been trained on\n",
    "- We underestimate the width of the corpus the LLM has been trained on and thus attribute reasoning and planning, when it is really just soft retrieval (in a sense there is no clear separation between train and test dataset)\n",
    "- Can't do reasoning\n",
    "  - No matter how computationally complex a problem is will always give answer after the same amount of time\n",
    "  - In reality gives approximate answers from its knowledge base\n",
    "- Thus not reasoning but retrieving\n",
    "  - E.g explain jokes but someine has explained joke in internet\n",
    "  - Good at cypher text one two and 13 what you can find on the internet..\n",
    "\n",
    "\n",
    "Large language models are thus good for idea generation, but they are not good for reasoning (they are akin to system I but are lacking system II)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrzALLPTCu-2"
   },
   "source": [
    "A recent paper termed paper [System 2 Attention](https://arxiv.org/pdf/2311.11829v1.pdf) thus introduces a system II into large language models\n",
    "\n",
    "- Regenerating the input context to only include the relevant portions, using the LLM itself as a natural language reasoner. This step is done by giving the LLM a zero-shot prompt that instructs it to perform the desired attention task over the input.\n",
    "- Attending to the regenerated context to elicit the final response, using the LLM again with another zero-shot prompt that asks for the answer to the original query\n",
    "\n",
    "Howerver, in the paper this is mainly done by extending the prompt, so this would not be a form of system 2 according to  the talk of Subbarao Kambhampati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "803fG9ivwoPJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

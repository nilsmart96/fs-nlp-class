{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNKT9+p7UJc5MYlF+d4As35"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The task\n","\n","Supposed you have a fixed computational budegt of GPU time and you want to know how to most effectively use it for you taks  - how would you approach this problem?\n","\n","Alternatively, supposed you can invest in colleting more training data or training a larger model- what is the most effective investment?\n"],"metadata":{"id":"XM1GS2Vl8wRj"}},{"cell_type":"markdown","source":["# Definition\n","\n","\n","**Scaling laws:** how the performance of deep lerning models is related to the\n","\n","a) Size of the model\n","\n","b) Size and variety of the dataset, and\n","\n","c) Amount of computation used for training\n","\n"],"metadata":{"id":"i4t0Xc8asoax"}},{"cell_type":"markdown","source":["# Motivation\n","\n","-> Essential in foundational models - can't try different combinations many times (time, cost, compute constraints)\n","\n","-> e.g LLMs\n","\n","\n","**Efficiency:** efficient allocation of resources by indicating which factor (model size, data size, or compute) is most likely to yield performance improvements.\n","\n","**Predictability:** provide a predictable framework for understanding how much improvement one can expect from scaling up each of the three above\n","\n","**Cost-Benefit Analysis:** conduct a cost-benefit analysis, optimizing the trade-off between the performance gains and the costs associated with larger models, more data, or increased computation.\n","\n","Scaling laws can be architecture dependent, but there is research that beyond architecture for general\n","\n","**Findings on scaling laws motivated Open AI to build Chat GPT! - Basically no architectural innovation but a bigger model with more data and computer!**"],"metadata":{"id":"QQFzOEpDuB6J"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Yx6g3_wL8i4j"}},{"cell_type":"markdown","source":["# Key points\n","\n","**Power-Law Scaling of Performance:** Many studies have found that performance improvements due to increasing model size, data size, or training computation often follow a power-law or sublinear scaling -> diminishing returns  (but also increasing accuracy with size)\n","\n","**Bigger Models Generalize Better:** Larger models not only fit the training data better but also tend to generalize better -> against traditional computationnal theory\n","\n","\n","**Optimal Compute-Data Balance:** optimal ratio of compute to data size that yields the most cost-effective improvements in performance. Adding more data without increasing compute can be less effective, and vice versa.\n","\n","**Transferability Across Tasks:** benefits of scaling often transfer across different tasks, model that's larger and performs better on one task is likely to do the same on another task, even if the tasks are quite different.\n","\n","**Model Scaling Is Task-Dependent:** The gains from scaling up the model size can be significantly different from one task to another.\n","\n","**Saturation Points:** For any given task, there seems to be a saturation point beyond which increasing model size yields minimal improvements.\n","\n","\n","\n","\n"],"metadata":{"id":"ZDS9NITmwtFU"}},{"cell_type":"markdown","source":["**Power law**\n","\n","A power law is a functional relationship between two quantities, where a **relative change in one quantity results in a proportional relative change in the other quantity, independent of the initial size of those quantities**: one quantity varies as a power of another. In its simplest form, a power law can be written as:\n","$$\n","y=k x^\\alpha\n","$$\n","where:\n","- $y$ and $x$ are the quantities of interest,\n","- $k$ is a constant coefficient,\n","- $\\alpha$ is the power law exponent."],"metadata":{"id":"GPOJoyuZWMdd"}},{"cell_type":"markdown","source":["### The original paper motivating the training of large language models\n","[**Kaplan et al - Scaling Laws for natural language models\n","Kaplan et al. (2020)**](https://arxiv.org/pdf/2001.08361.pdf)\n","\n","\n","- Large models should not be trained to their lowest possible loss to be\n","compute optimal.\n","- Dataset to training parameter size roughly 2:1 (see for example Chat GPT)\n","\n","\n","<a href=\"https://arxiv.org/pdf/2001.08361.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1y60qQpZB7_SF59Hhp2m2VeRV5fxmhrw9\" width=60%></a>\n","\n","\n"],"metadata":{"id":"_3KEAPhvcHHG"}},{"cell_type":"markdown","source":["[**Given a fixed FLOPs budget, how should one trade-off model\n","size and the number of training tokens?**](https://arxiv.org/pdf/2203.15556.pdf)\n","\n","- However, later papers (e.g. the one above) find a different relationship, where ther ratio of data to parameters should be larger\n","\n","- Maintain though that modles should be trained to their lowest possible loss\n","\n","- Training over 400 language models ranging from 70 million to over 16 billion\n","parameters on 5 to 500 billion tokens\n","\n","\n","\n","\n","<a href=\"https://arxiv.org/pdf/2203.15556.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1BObhF1nsNFoIeu5I-xVcN0FwabpqamI2\" width=60%></a>\n","\n","\n"],"metadata":{"id":"koiURmW075Xl"}},{"cell_type":"markdown","source":["- find that for compute-optimal training, the model size and\n","the number of training tokens should be scaled equally: for every doubling of model size the number\n","of training tokens should also be doubled.\n","\n","\n","<a href=\"https://arxiv.org/pdf/2203.15556.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1PfWNsoqOVURyXfuXvLw9jvcMMh4G9k9g\" width=60%></a>\n","\n"],"metadata":{"id":"A_VaC9ZUMlhZ"}},{"cell_type":"markdown","source":["- Test hypothesis by training a predicted computeoptimal\n","model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4 times more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B),\n","Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.\n","\n","\n","<a href=\"https://arxiv.org/pdf/2203.15556.pdf\"><img src=\"https://drive.google.com/uc?export=view&id=1si1wNSxWkgb4ZMNBFGpfSbeG1nxp84En\" width=60%></a>\n","\n","\n","\n","\n","\n","As a highlighted, Chinchilla **reaches a state-of-the-art average accuracy of\n","67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher**\n","reduced\n","model size reduces inference cost considerably and greatly facilitates downstream uses on smaller\n","hardware."],"metadata":{"id":"Uf4ADXkMNw3D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pd_DvaekskNF"},"outputs":[],"source":[]}]}
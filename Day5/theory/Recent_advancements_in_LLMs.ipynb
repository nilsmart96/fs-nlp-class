{"cells":[{"cell_type":"markdown","metadata":{"id":"Cn7Tm4K8ptuJ"},"source":["# \"Recent advancements\""]},{"cell_type":"markdown","metadata":{"id":"002oa8JTptuf"},"source":["The availability of \"Gigantic Language Models\" (GLMs) like GPT-3 whose weights are very difficult to modify raises interesting questions about the optimal way of using them for downstream tasks, in particular, for supervised problems.\n","\n","Two paradigms seem to be emerging:\n","\n","- __Prompt engineering/generation__: Try to find the the best GLM prompt(-function) and answer-transformation function for few-shot learning on the task -- solve the problem directly without training/fine-tuning.\n","- __Try to generate some useful intermediary artifacts__ which then can be used to train/fine-tune other models."]},{"cell_type":"markdown","metadata":{"id":"lA1JT92Bptuk"},"source":["## Prompt-engineering/generation methods\n","\n","A recently published survey, [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing (2021)](https://arxiv.org/abs/2107.13586), gives a detailed overview of the prompt engineering approaches that have been developed for using GLMs to solve supervised problems.\n","\n","Main points:\n","\n","There are two prompt formats, corresponding to two main types of pretrained LMs:\n","- __Prefix prompts__, for LMs that __continue__ a prompt when they generate language: these are for \"traditional\" causal/autoregressive LMs generating continuation probabilites, e.g. GPT-x, T5 etc.\n","- __Cloze prompts__, for LMs using masked objective variants: in this case prompt contains some blank parts that model has to fill in, e.g., BERT, RoBERTa etc."]},{"cell_type":"markdown","metadata":{"id":"6ySRCBk-ptup"},"source":["Prompt creation methods for a given supervised task:\n","\n","- __Manual prompt template engineering__ is pretty widespread, based on expertise and an understanding of both the model architecture, pretraining method and the task.\n","\n","A concrete example for semantic parsing ([source paper](https://arxiv.org/pdf/2104.08768v1.pdf)):\n","\n","> Letâ€™s translate what a human user says into what a computer might say.\n","\n","> Human: when is the weekly standup\n","\n","> Computer: start time of weekly standup\n","\n","> Human: what date is the weekly standup\n","\n","> Computer: date of weekly standup\n","\n","> ...\n","\n","> Human: how long is the weekly standup\n","\n","> Computer:\n","\n","> ____"]},{"cell_type":"markdown","metadata":{"id":"7AKo48TSptuu"},"source":["### Automatic prompt generation\n","\n","But how do we know that these prompts are optimal? We don't, so let's try to __optimize__ prompt generation! Existing implementations of this idea:\n","\n","__Discrete prompts:__\n","- __prompt mining:__ scrape a large corpus and find __middle words or dependency paths__ (!) between the xs and ys of the example pairs, and use this to form the prompt.\n","- __prompt paraphrasing__: start with a seed prompt template, generate paraphrases, and then choose the highest performing variant.\n","- __gradient-based search__ for optimal textual prompts.\n","- __template generation__: you can try to train text generators, e.g. T5 to generate prompt templates based on examples...\n","\n","__Prompting directly in the embedding space:___\n","\n","- The core idea is to use continuous embeddings as prompts -- this opens the possibility of actually using GD-optimized networks to produce the continuous prompts from the text input and even using non-textual inputs like image embeddings,for, e.g., image captioning."]},{"cell_type":"markdown","metadata":{"id":"NlL5O2CQptux"},"source":["## Zero-label learning with synthetic examples (Google, Sep. 2021)\n","\n","(The paper can be found [here](https://arxiv.org/pdf/2109.09193.pdf))\n","\n","__Main motivation:__\n","- few-shot learning GLM performance is still worse than fine-tuning \"small\" models, e.g., fine-tuned T5 beats few-shot learning using GPT-3 on..."]},{"cell_type":"markdown","metadata":{"id":"c_tiexuuptu0"},"source":["__SuperGLUE:__ a very challenging, recent (2019) NLU dataset consisting of\n","- WSD,\n","- coreference resolution\n","- question answering, and\n","- natural language inference\n","tasks.\n","\n","<img src=\"https://miro.medium.com/max/667/1*I5m7DxaB7af8d189drFixg.png\">\n","\n","(Figure from the [paper](https://arxiv.org/abs/1905.00537).)"]},{"cell_type":"markdown","metadata":{"id":"dgFsdhWvptu4"},"source":["__Performance on SuperGLUE__\n","\n","71.8 vs 89.3% avg. performance on SuperGLUE for fine-tuned T5 vs few-shot learning with GPT-3, human performance is\n","89.8."]},{"cell_type":"markdown","metadata":{"id":"hBK4VIwVptu7"},"source":["__Approach__:\n","- use a GLM for __synthetic datapoint generation__ and then use the synthetically generated dataset to fine-tune a \"small\" pretrained LM like T5 or BERT for the task.\n","- with appropriate prompt engineering, you don't need labels(!) you just prompt the GLM to produce X datapoints for given Ys -- the whole procedure is \"zero-label\"."]},{"cell_type":"markdown","metadata":{"id":"mX9n3zJPptu9"},"source":["Figure from the paper:\n","<img src=\"https://miro.medium.com/max/933/0*xQI2ysY_loXMTlmb.png\" widt=\"100%\">"]},{"cell_type":"markdown","metadata":{"id":"7qVdNCFnptvA"},"source":["__Experiments and results__:\n","- Used Yelp and Amazon sentiment classification datasets (only the Xs!) and DBPedia topic classification\n","- Generated 10000 synthetic examples per class for the class. problems\n","- top K sampling for generation\n","- __T5 fine-tuned with the synthetic data:__ new state of the art results for zero/few shot learning (without labels!!)\n","- used for __SuperGLUE__ as __a data augmentation method__, the __fine-tuned T5 model__ achieved -- for the first time -- a better then human performance (90.4 percent)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}